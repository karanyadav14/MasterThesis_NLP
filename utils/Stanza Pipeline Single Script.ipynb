{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312c5956",
   "metadata": {},
   "source": [
    "## 0. Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce91b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script/ Sp1 overlap count is 178\n",
      "Script/ Sp2 overlap count is 174\n",
      "E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/SingleScript/temp_files\n",
      "Script/ Sp1 overlap count is 146\n",
      "Script/ Sp2 overlap count is 142\n",
      "E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/SingleScript/temp_files\n",
      "ERROR FOR overlap: pt/hi_6 00:08:21.849 # ठीक है # वो उस का # हस्बेंड अगर चाहता है जैसे रखना चाहता है वैसे रहे तो ठीक है । # मतलब कम से कम # \\r उस का \\r # माँ की आदत # वादत छोड़ी है मतलब छूट ही जाता है शादी के बाद  ।\n",
      "ERROR FOR overlap: pt/hi_6 00:16:10.331 हाँ । \\r नहीं \\r अब मतलब वहाँ ही रहना ही है । # मकान तुम्हारा है ही । तो # तुम्हें मतलब वहाँ का ले के ही रहना चाहिए # ।\n",
      "ERROR FOR overlap: pt/hi_6 00:18:35.653 कि नहीं । अब \\r 10 1 द \\r 1 हफ़्ते इस इंटर्व्यू के लिए इंतज़ार करना है । 1 हफ़्ते अगले के लिए इंतज़ार करना है । # [laughter] फ़िर अगल ।\n",
      "ERROR FOR overlap: pt/hi_6 00:23:08.956 # यहाँ पे # ये बच्चे हैं ना । तो इन लोगों से ही तुम मतलब # पता कर सकती हो ना कि हम लोग हैं कहाँ ।\n",
      "ERROR FOR r: pt/hi_6 00:25:30.307 \\r तुम क्या कह रही \\r तुम ने कार्ड \\r तुम्हारा आया तो है कार्ड मेरे पास ।\n",
      "Script/ Sp1 overlap count is 388\n",
      "Script/ Sp2 overlap count is 372\n",
      "E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/SingleScript/temp_files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "def checks(file_path):\n",
    "    '''This script checks the CSV file for the follwing errors:\n",
    "            # check for pipe symbol instead of hindi purnaviram symbol and undo if found \n",
    "            # pairing of annotation tags\n",
    "            - even number of instances in each utterance \n",
    "            - if not, print the error with filename, begin_time, utterance\n",
    "            # the overlap tag should be equal in number for Sp1 and Sp2 - print the no. of # for Sp1 and Sp2\n",
    "            '''\n",
    "    \n",
    "    dirs = os.listdir(file_path)\n",
    "    #print(dirs)\n",
    "    # for every file in the directory\n",
    "    for file in dirs:\n",
    "        filename = file_path+str(file)\n",
    "        #print(file)\n",
    "\n",
    "        if filename.endswith(\"_0.csv\"):\n",
    "            filename_in = filename\n",
    "           \n",
    "            #print (\"in = \"+ filename_in)\n",
    "            #print (\"out = \"+ filename_out)\n",
    "\n",
    "            fields = []\n",
    "            rows = []\n",
    "            column_value = \"\" #variable for the string in the transcription column\n",
    "            data_value= \"\" #variable for the string in the transcription column as it will appear in the output file\n",
    "            speaker_value = \"\" #variable for the string value of Speaker1 or Speaker2\n",
    "            result_list = []\n",
    "\n",
    "            count_overlap_sp1 = 0\n",
    "            count_overlap_sp2 = 0\n",
    "\n",
    "            with open(filename_in, 'r', encoding=\"utf-8\") as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "                #print (filename_in)\n",
    "\n",
    "                for row in reader:\n",
    "\n",
    "                    begin_time = row[\"Begin Time - hh:mm:ss.ms\"]\n",
    "                    end_time = row[\"End Time - hh:mm:ss.ms\"]\n",
    "                    duration = row[\"Duration - hh:mm:ss.ms\"]\n",
    "                    speaker_1 = row[\"Channel1\"]\n",
    "                    speaker_2 = row[\"Channel2\"]\n",
    "\n",
    "\n",
    "                    # the last above line assigns str in \"data\" column to variable data_value. \n",
    "                    # Also replaces a double white space with a single white space \n",
    "                    # Replaces pipe symbol with hindi purnaviram symbol\n",
    "                    if speaker_1 !=\"\":\n",
    "                        data_value=speaker_1+str(\" ।\");     #Adding Purnaviram to see how pipeline works when \n",
    "                    else:                                  #transcription already have it   \n",
    "                        data_value=speaker_2+str(\" ।\");\n",
    "\n",
    "                    data_value = str(data_value).replace(\"  \",\" \").replace(\"|\",\"।\")\n",
    "                    #count the number of \\d in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_d = int(data_value.count(r\"\\d\"))\n",
    "                    if (count_d % 2) != 0:   \n",
    "                        print (\"ERROR FOR d: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\r in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_r = int(data_value.count(r\"\\r\"))\n",
    "                    if (count_r % 2) != 0:   \n",
    "                        print (\"ERROR FOR r: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\h in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_h = int(data_value.count(r\"\\h\"))\n",
    "                    if (count_h % 2) != 0:   \n",
    "                        print (\"ERROR FOR h: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\exp in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_exp = int(data_value.count(r\"\\exp\"))\n",
    "                    if (count_exp % 2) != 0:   \n",
    "                        print (\"ERROR FOR exp: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\q in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_q = int(data_value.count(r\"\\q\"))\n",
    "                    if (count_q % 2) != 0:   \n",
    "                        print (\"ERROR FOR q: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\c in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_c = int(data_value.count(r\"\\c\"))\n",
    "                    if (count_c % 2) != 0:   \n",
    "                        print (\"ERROR FOR c: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of # in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_overlap = int(data_value.count(r\"#\"))\n",
    "                    if (count_overlap % 2) != 0:   \n",
    "                        print (\"ERROR FOR overlap: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #to check overlaps - additional check - for the 2 speakers, the total number of overlap tags should be the same\n",
    "                    if speaker_1 != \"\":\n",
    "                        count_overlap_sp1 = count_overlap_sp1 + count_overlap\n",
    "                    if speaker_2 != \"\":\n",
    "                        count_overlap_sp2 = count_overlap_sp2 + count_overlap\n",
    "\n",
    "                    if speaker_1 !=\"\":\n",
    "                        speaker_1=data_value;\n",
    "                    else:\n",
    "                        speaker_2=data_value;\n",
    "                        \n",
    "                        \n",
    "                    speaker_1 = str(speaker_1).replace(\"  \",\" \").replace(\"|\",\"।\")\n",
    "                    speaker_2 = str(speaker_2).replace(\"  \",\" \").replace(\"|\",\"।\")\n",
    "\n",
    "                    result_list.append([begin_time, end_time, duration, speaker_1, speaker_2])\n",
    "                    #print(result_list)\n",
    "\n",
    "            #the following prints for each file, the total number of overlap tags for Sp1 and Sp2 \n",
    "            print (str(filename_in[-20:-13]) + \" Sp1 overlap count is \" + str(int(count_overlap_sp1)))\n",
    "            print (str(filename_in[-20:-13]) + \" Sp2 overlap count is \" + str(int(count_overlap_sp2)))\n",
    "\n",
    "               \n",
    "            \n",
    "            \n",
    "            #Create a New folder to store output of intermediatory steps\n",
    "            directory= \"temp_files\" \n",
    "            new_path=os.path.join(file_path,directory)\n",
    "            print(new_path)\n",
    "            try:\n",
    "                os.makedirs(new_path,exist_ok=True)\n",
    "            except OSError as error:\n",
    "                pass\n",
    "\n",
    "\n",
    "            new_filename = file.replace(\"_0.csv\", \"_0_checks.txt\")\n",
    "            filename_out2 = os.path.join(new_path,new_filename)\n",
    "            #print(filename_out2)\n",
    "\n",
    "\n",
    "            \n",
    "                \n",
    "            with open(filename_out2, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "                # creating a csv writer object\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                # writing the fields\n",
    "                fields = [\"Begin Time - hh:mm:ss.ms\",\"End Time - hh:mm:ss.ms\",\"Duration - hh:mm:ss.ms\", 'Channel1','Channel2']\n",
    "                csvwriter.writerow(fields)\n",
    "                csvwriter.writerows(result_list)\n",
    "\n",
    "                \n",
    "#Giving directory path of csv input files to checks function                \n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/SingleScript/\"\n",
    "checks(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8871bb",
   "metadata": {},
   "source": [
    "## Single Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c66ef6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import requests\n",
    "import pyconll\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.utils.conll import CoNLL\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        \n",
    "        \n",
    "    def format1(self):\n",
    "        \n",
    "        ''' # formats the output of Checks steo to add Speaker_id (Sp1,Sp2 and if present Sp3)\n",
    "            # merges two Channel columns (for two speakers) to one data column\n",
    "            # adds purna viram to the end of every utterance\n",
    "            #   Input file = hi_1234_0_checks.txt  \n",
    "            #   Output file =  hi_1234_1_format.txt\n",
    "        '''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "            \n",
    "            if filename.endswith(\"_0_checks.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_0_checks.txt\",\"_1_format.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                fields = []\n",
    "                rows = []\n",
    "                column_value = \"\" #variable for the string in the transcription column\n",
    "                data_value= \"\" #variable for the string in the transcription column as it will appear in the output file\n",
    "                speaker_value = \"\" #variable for th string value of Speaker1 or Speaker2\n",
    "                result_list = []  \n",
    "\n",
    "                with open(filename_in, 'r', encoding=\"utf-8\") as csvfile:\n",
    "                    csvreader = csv.reader(csvfile)\n",
    "                    reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "                    #print (filename_in)\n",
    "                    \n",
    "                    flag=False\n",
    "                    flag1=False\n",
    "                    \n",
    "                    for row in reader:\n",
    "\n",
    "                        begin_time = row[\"Begin Time - hh:mm:ss.ms\"]\n",
    "                        end_time = row[\"End Time - hh:mm:ss.ms\"]\n",
    "                        duration = row[\"Duration - hh:mm:ss.ms\"]\n",
    "                        speaker_1 = row[\"Channel1\"]\n",
    "                        speaker_2 = row[\"Channel2\"]\n",
    "                        \n",
    "                        #print(speaker_1, len(speaker_1), \"-\", speaker_2, len(speaker_2))\n",
    "                        if len(str(speaker_1).strip()) == 0:    # use strip in case there are empty spaces at the end of the string\n",
    "                            column_value = str(speaker_2).strip()    #if empty then this col will be filled with speaker2 value \n",
    "                            if \"[b_speaker3]\" in (column_value):\n",
    "                                flag=True\n",
    "                                speaker_value=\"Sp3\"\n",
    "\n",
    "                            elif \"[e_speaker3]\" in (column_value) :\n",
    "                                flag=False\n",
    "                                speaker_value=\"Sp3\"\n",
    "\n",
    "                            elif flag==True:\n",
    "                                speaker_value=\"Sp3\"\n",
    "                            else:\n",
    "                                speaker_value = \"Sp2\"       # correct speaker id is inserted\n",
    "                        elif len(str(speaker_2).strip()) == 0:\n",
    "                            column_value = str(speaker_1).strip()\n",
    "                            if \"[b_speaker3]\" in (column_value):\n",
    "                                flag1=True\n",
    "                                speaker_value=\"Sp3\"\n",
    "                            elif flag1==True:\n",
    "                                speaker_value=\"Sp3\"\n",
    "                            elif \"[e_speaker3]\" in (column_value) :\n",
    "                                flag1=False\n",
    "                                speaker_value=\"Sp3\"   \n",
    "                            else:\n",
    "                                speaker_value = \"Sp1\"\n",
    "#                         data_value = column_value+str(\" ।\")     #add a period marker at the end of every utterance (REMOVE)\n",
    "                        #This last above step can be omitted if the sentence segmentation is taken care of in the transcription\n",
    "                        \n",
    "                        if \"[b_speaker3]\" in column_value:\n",
    "                            column_value=column_value.replace(\"[b_speaker3]\",\" \").replace(\"  \",\"\")\n",
    "\n",
    "                        elif \"[e_speaker3]\" in column_value:\n",
    "                            column_value=column_value.replace(\"[e_speaker3]\",\"\").replace(\"  \",\" \")\n",
    "\n",
    "\n",
    "                        data_value = column_value\n",
    "                        #print(data_value)\n",
    "        \n",
    "                        result_list.append([begin_time, end_time, duration, speaker_value, data_value])\n",
    "                        #print(result_list)\n",
    "\n",
    "                # writing to csv file\n",
    "                with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "                # creating a csv writer object\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                # writing the fields\n",
    "                    fields = ['Begin_Time-hh:mm:ss.ms', 'End_Time-hh:mm:ss.ms', 'Duration-hh:mm:ss.ms', 'speaker_id', 'data']\n",
    "                    csvwriter.writerow(fields)\n",
    "                    csvwriter.writerows(result_list)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "    def segment(self):\n",
    "        \n",
    "        ''' # one word in a row with space as a delimiter (including annotation tags)\n",
    "            # each token/word given an index\n",
    "            # sentence segmentation with purnaviram as delimiter\n",
    "            # sent_id added to each sentence\n",
    "            # Metadata added as comments before every sentence: \n",
    "               - begin_time, end_time, duration, speaker_id, contains_overlap\n",
    "            # contains_overlap = True/False (removes # from tokens)\n",
    "            # insert blank row after every sentence (after every PUNCT)\n",
    "            #   Input file = hi_1234_1_format.txt  \n",
    "            #   Output file =  hi_1234_2_segment.txt\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "            \n",
    "            if filename.endswith(\"_1_format.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_1_format.txt\", \"_2_segment.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                fields = []\n",
    "                rows = []\n",
    "                result_list = []\n",
    "                sent_id = 1\n",
    "\n",
    "                with open(filename_in, 'r',encoding=\"utf-8\") as csvfile:\n",
    "                    csvreader = csv.reader(csvfile)\n",
    "                    reader = csv.DictReader(csvfile, delimiter=',')\n",
    "\n",
    "                    result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "\n",
    "                    for row in reader:\n",
    "                        word_index = 1\n",
    "                        file_name= filename_in[-16:-9]\n",
    "                        begin_time = row[\"Begin_Time-hh:mm:ss.ms\"]\n",
    "                        end_time = row[\"End_Time-hh:mm:ss.ms\"]\n",
    "                        duration = row[\"Duration-hh:mm:ss.ms\"]\n",
    "                        speaker_name = row[\"speaker_id\"]\n",
    "                        rowdata = str(row['data']).strip()\n",
    "                        word_list = rowdata.split(' ')\n",
    "                        \n",
    "                        flag = False\n",
    "                        var= False\n",
    "\n",
    "                        count = 0\n",
    "                        count_list = [\"#\", ]\n",
    "                        N = len(word_list)\n",
    "                        temp_list = []\n",
    "        #                 print(temp_list)\n",
    "                        for word in word_list:\n",
    "        #                     if not word.startswith((\"#\", \"\\\\\",)):\n",
    "        #                         print(\"###\")                   \n",
    "        #                         count+=1\n",
    "        #                     print(word)\n",
    "                            #if word != \"\": #this line creates a check for double spaces - not required now, as double speces eliminated by checks.py\n",
    "                            if word == \"#\":\n",
    "                                if flag == False:\n",
    "                                    flag = True     \n",
    "                                else:\n",
    "                                    flag = False\n",
    "                                    var = True\n",
    "\n",
    "\n",
    "                            elif word == \"।\":\n",
    "                                temp_list.append([word_index, word])\n",
    "\n",
    "                                result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "                                result_list.append([(\"# begin_time = \" + str(begin_time)), \"\"])\n",
    "                                result_list.append([(\"# end_time = \" + str(end_time)), \"\"])\n",
    "                                result_list.append([(\"# duration = \" + str(duration)), \"\"])\n",
    "                                result_list.append([(\"# speaker_id = \" + str(speaker_name)), \"\"])\n",
    "                                if flag ==True:\n",
    "                                    x= True\n",
    "                                elif var ==True:\n",
    "                                    x= True\n",
    "                                    var = False\n",
    "                                else:\n",
    "                                    x=False\n",
    "                                result_list.append([(\"# contains_overlap = \"+str(x)), \"\"])\n",
    "                                result_list.extend(temp_list)\n",
    "                                result_list.append([\"\", \"\"])\n",
    "\n",
    "                                temp_list = []\n",
    "                                word_index = 1\n",
    "                                sent_id = sent_id+1    \n",
    "\n",
    "                            else:\n",
    "                                temp_list.append([word_index, word])\n",
    "        #                         print(temp_list)\n",
    "                                word_index = word_index+1\n",
    "\n",
    "\n",
    "                with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "                    # creating a csv writer object\n",
    "                    csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "                    csvwriter.writerows(result_list)\n",
    "        \n",
    "\n",
    "    def chunks(self):\n",
    "        \n",
    "        \n",
    "        ''' # creates chunks based on (outermost) annotation tags \n",
    "            # re-orders the word indices within sentences after chunking\n",
    "            # adds prefix and suffix of outermost tag to the chunk \n",
    "            # creates a misc string for outermost tag\n",
    "            # nested tags are included in the chunk as is\n",
    "            # maintain the structure of metadta in comments\n",
    "            #   Input file =  hi_1234_2_segment.txt \n",
    "            #   Output file =  hi_1234_3_chunks.txt\n",
    "        '''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "        \n",
    "            if filename.endswith(\"_2_segment.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_2_segment.txt\", \"_3_chunks.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                result_list= []\n",
    "                sent_id = 1\n",
    "                chunk_list = []\n",
    "                chunk = \"\"\n",
    "                chunk_id = \"\"\n",
    "                word_index = 1\n",
    "                misc = {}\n",
    "                with open(filename_in,'r', encoding = 'utf-8') as csvfile:\n",
    "                    csvreader = csv.reader(csvfile, delimiter='\\t')\n",
    "\n",
    "                    for row in csvreader:  \n",
    "                        word_n = row[0]\n",
    "                        #print(word_n)\n",
    "                        word_form = row[1]\n",
    "                        #print(word_form)\n",
    "                        #print(type(word_form))\n",
    "                        if len(chunk_list) != 0 and word_form[0]!=\"\\\\\":\n",
    "                            chunk_list.append(word_form)\n",
    "\n",
    "                        if word_n.startswith(\"#\"):\n",
    "                            result_list.append([word_n,\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"])\n",
    "                            continue\n",
    "                        elif word_n == \"\":\n",
    "                            continue \n",
    "                        elif word_form ==\"।\":\n",
    "                            if len(chunk_list) != 0:\n",
    "                                chunk_list.append(chunk_id)\n",
    "                                chunk = \"_\".join(chunk_list)\n",
    "                                #chunk = chunk.replace(\"\\\\\",\"\")\n",
    "                                result_list.append([str(word_index),chunk,misc])\n",
    "                                word_index = word_index + 1\n",
    "                                result_list.append([str(word_index),word_form,\"_\"])\n",
    "                                result_list.append([\"\",\"\",\"\"])\n",
    "                                word_index = 1\n",
    "                                chunk_list =[]  \n",
    "                                #this block should check instances of errors where tag is not closed. This code should serve as a barrier for the error to carry over beyond sentence boundary.\n",
    "                                #check in the end if this block works, by manually removing one of the closing tags (outermost, not nested) in a sentence\n",
    "                            else:\n",
    "                                result_list.append([str(word_index),word_form,\"_\"])\n",
    "                                result_list.append([\"\",\"\",\"\"])\n",
    "                                word_index = 1  \n",
    "\n",
    "                        elif word_form.startswith(\"\\\\\"):\n",
    "                            if len(chunk_list) == 0:\n",
    "                                chunk_id = word_form\n",
    "                                chunk_list.append(word_form)\n",
    "                                \n",
    "                                 \n",
    "                                if word_form[1]=='q':\n",
    "                                    misc = \"Quote=Matrix_Tag\"\n",
    "                                if word_form[1]=='c': \n",
    "                                    misc = \"CodeSwitch=Matrix_Tag\"\n",
    "                                if word_form[1]=='r':\n",
    "                                    misc = \"Repair=Matrix_Tag\"\n",
    "                                if word_form[1]=='d':\n",
    "                                    misc = \"Disfluency=Matrix_Tag\"\n",
    "                                if word_form[1]=='h':\n",
    "                                    misc = \"Hesitation=Matrix_Tag\"\n",
    "                                if word_form[1]=='e':  \n",
    "                                    misc = \"Expletive=Matrix_Tag\"\n",
    "\n",
    "                                #print(misc)\n",
    "                            elif word_form == chunk_id:\n",
    "                                chunk_list.append(word_form)\n",
    "                                chunk = \"_\".join(chunk_list)\n",
    "                                #chunk = chunk.replace(\"\\\\\",\"\")\n",
    "                                result_list.append([str(word_index),chunk,misc])\n",
    "                                word_index = word_index +1\n",
    "                                chunk_list =[]\n",
    "                            else:\n",
    "                                chunk_list.append(word_form)\n",
    "                            continue                            \n",
    "\n",
    "                        else:\n",
    "                            if len(chunk_list) == 0:\n",
    "                                result_list.append([str(word_index) ,word_form,\"_\"])\n",
    "                                word_index = word_index + 1\n",
    "\n",
    "                with open(filename_out, 'w',encoding = 'utf-8', newline='') as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "                    csvwriter.writerows(result_list)\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "    def misc(self):\n",
    "        \n",
    "        ''' # maintains the sent_id and word_index of the tokens\n",
    "            # for each nested tag, adds information to misc column\n",
    "            # removes tags from the chunks\n",
    "            # format of chunk: prefix & suffix \"_\" retained (is this needed?)\n",
    "            # formats the output to conllu format \n",
    "            # maintains the structure of metadata in comments\n",
    "            #   Input file =  hi_1234_3_chunks.txt\n",
    "            #   Output file =  hi_1234_4_misc.txt\n",
    "        '''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "        \n",
    "            if filename.endswith(\"_3_chunks.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_3_chunks.txt\", \"_4_misc.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "               \n",
    "                with open(filename_in,'r', encoding = 'utf-8') as csvfile:\n",
    "                    csvreader = csv.reader(csvfile, delimiter='\\t' )\n",
    "\n",
    "                    result_list = []\n",
    "\n",
    "                    for row in csvreader: \n",
    "                        \n",
    "\n",
    "                        word_n = row[0]\n",
    "                        word_form = row[1]\n",
    "                        misc = row[2]\n",
    "                        misc_list= []\n",
    "\n",
    "                        if word_n.startswith(\"#\"):\n",
    "                            result_list.append(word_n+'\\n')\n",
    "                            continue\n",
    "\n",
    "                        elif word_n ==\"\":\n",
    "                            result_list.append('\\n')  \n",
    "                            continue\n",
    "\n",
    "                        elif word_form.startswith(\"\\\\\"):\n",
    "                            misc_list.append(misc)\n",
    "                            if \"_\\\\q_\" in word_form or \"_\\\\r_\" in word_form or \"_\\\\d_\" in word_form or \"_\\\\h_\" in word_form or \"_\\\\c_\" in word_form or \"_\\\\exp_\" in word_form:\n",
    "                                misc_list.append(\"NestedTag=True\")\n",
    "                                #misc_list.append({\"NestedTag\":\"True\"})\n",
    "                                if \"_\\\\q_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\q\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"Quote='\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\r_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\r\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"Repair='\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\d_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\d\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"Disfluency='\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\h_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\h\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"Hesitation='\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\c_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\c\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"CodeSwitch='\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\exp_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\exp\")[1].strip(\"_\")   #\n",
    "                                    misc_list.append(\"Expletive='\"+subtag+\"'\")\n",
    "                                    #print (subtag)   \n",
    "                                misc = r\"|\".join(misc_list) \n",
    "                                #print (type(misc))\n",
    "                                #print(misc_list)\n",
    "                                word_form = word_form.replace(\"\\\\q_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\q\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\r_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\r\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\d_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\d\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\h_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\h\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\c_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\c\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\exp_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\exp\", \"\")\n",
    "                                word_form = word_form.replace(\"__\", \"_\")\n",
    "\n",
    "                                result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+misc+'\\n')\n",
    "                            else:\n",
    "                                word_form = word_form.replace(\"\\\\q_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\q\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\r_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\r\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\d_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\d\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\h_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\h\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\c_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\c\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\exp_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\exp\", \"\")\n",
    "                                word_form = word_form.replace(\"__\", \"_\")\n",
    "\n",
    "                                result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+misc+'\\n')    \n",
    "                        else:\n",
    "                            result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\n')\n",
    "\n",
    "\n",
    "                with open(filename_out, 'w', encoding = 'utf-8') as f:\n",
    "                        for item in result_list:\n",
    "                            f.write(item)\n",
    "                        \n",
    "    def stanza_pos(self):\n",
    "            \n",
    "        ''' This function uses Stanza POS tagger for POS tagging \n",
    "            #   Input file =  hi_1234_4_misc.txt\n",
    "            #   Output file =  hi_1234_5_pos.txt\n",
    "         '''\n",
    "\n",
    "        #giving the path of the directory\n",
    "        dirs = os.listdir(self.filepath)\n",
    "\n",
    "        # for every file in the directory\n",
    "        for file in dirs:\n",
    "            filename = path+str(file)\n",
    "\n",
    "            if filename.endswith(\"_4_misc.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_4_misc.txt\", \"_5_pos.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "\n",
    "                doc = CoNLL.conll2doc(filename_in)   #Coverting txt file to stanza Document\n",
    "                #print(type(doc))\n",
    "\n",
    "                #Create Stanza POS tagging pipeline\n",
    "                pos_pipeline = stanza.Pipeline(lang='hi', processors='tokenize,lemma, pos', tokenize_pretokenized=True)\n",
    "\n",
    "                pos_tagged_doc = pos_pipeline(doc)\n",
    "                # print(pos_tagged_doc)\n",
    "\n",
    "                CoNLL.write_doc2conll(pos_tagged_doc, filename_out)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "        \n",
    "    def stanza_pos_rules(self):\n",
    "        \n",
    "        ''' # The deterministic rules:\n",
    "            #   1. The following are tagged X in one step\n",
    "            #   * disfluency - prefix \"d_\"\n",
    "            #   * repair - prefix \"r_\"\n",
    "            #   * hesitation - prefix \"h_\"\n",
    "            #   * quote - prefix \"q_\"\n",
    "            #   * code_switching - prefix \"c_\"\n",
    "            #   * [pause]\n",
    "            #   * [aside]\n",
    "            #   * [b_aside]\n",
    "            #   * [e_aside]\n",
    "            #   * [laughter]\n",
    "            #   * [noise]\n",
    "            #   * [incomprehensible]\n",
    "            #   2. The following tags are modified, unless already tagged X in the previous step:\n",
    "            #   * हाँ > INTJ\n",
    "            #   * ह्म > PART\n",
    "            #   * [anonymized] >PROPN\n",
    "        \n",
    "            #   Input file =  hi_1234_5_pos.txt\n",
    "            #   Output file =  hi_1234_6_pos_rules.txt\n",
    "        '''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "            \n",
    "            if filename.endswith(\"_5_pos.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_5_pos.txt\", \"_6_pos_rules.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                file = pyconll.load_from_file(filename_in)\n",
    "\n",
    "                for sentence in file:\n",
    "                    for token in sentence:\n",
    "                        misc_item1 = list((token.misc).items())\n",
    "                        #print (misc_item1)\n",
    "                        tag = [('Quote', {'Matrix_Tag'}), ('Repair', {'Matrix_Tag'}), ('Disfluency', {'Matrix_Tag'}), \n",
    "                       ('Hesitation', {'Matrix_Tag'}),('CodeSwitch', {'Matrix_Tag'}), ('Expletive', {'Matrix_Tag'})]\n",
    "                        for i in tag:\n",
    "                            if i in misc_item1:\n",
    "                                token.upos =\"X\"\n",
    "                            #print(misc_item1)\n",
    "                            #print(token.upos)\n",
    "                        #help(token.misc)    \n",
    "                        \n",
    "                        if str(token.form) == \"[pause]\":\n",
    "                            token.upos = \"X\"\n",
    "                            \n",
    "                        if str(token.form) == \"[aside]\":\n",
    "                            token.upos = \"X\"\n",
    "\n",
    "                        if str(token.form) == \"[b_aside]\":\n",
    "                            token.upos = \"X\"\n",
    "\n",
    "                        if str(token.form) == \"[e_aside]\":\n",
    "                            token.upos = \"X\"\n",
    "                        \n",
    "                        if str(token.form) == \"[laughter]\":\n",
    "                            token.upos = \"X\"\n",
    "                            \n",
    "\n",
    "                        if str(token.form) == \"[incomprehensible]\":\n",
    "                            token.upos = \"X\"\n",
    "\n",
    "                        if str(token.form) == \"[noise]\":\n",
    "                            token.upos = \"X\"\n",
    "\n",
    "                        if str(token.form) == \"हाँ\":\n",
    "                            token.upos = \"INTJ\"          #Changed from PART to INTJ\n",
    "\n",
    "                        if str(token.form) == \"ह्म\":\n",
    "                            token.upos = \"PART\" \n",
    "\n",
    "                        if str(token.form) == \"[anonymized]\":\n",
    "                            token.upos = \"PROPN\"\n",
    "\n",
    "#                         if str(token.lemma) == \"है\" and str(token.xpos) == \"VM\":\n",
    "#                             token.upos = \"VERB\"\n",
    "\n",
    "#                         if str(token.lemma) == \"था\" and str(token.xpos) == \"VM\":\n",
    "#                             token.upos = \"VERB\"            \n",
    "\n",
    "                with open(filename_out, 'w', encoding = 'utf-8') as f:\n",
    "                    file.write(f)            \n",
    "\n",
    "    \n",
    "    def sent_comment(self):\n",
    "        \n",
    "        ''' This function adds sentence as a meta data   \n",
    "        '''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "\n",
    "        # for every file in the directory\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "\n",
    "            if filename.endswith(\"_6_pos_rules.txt\"):\n",
    "    \n",
    "                file = pyconll.load_from_file(filename)  # Loading file using pyconll\n",
    " \n",
    "                sent = []                          # Creating list to form a single sentence \n",
    "\n",
    "                for sentence in file:\n",
    "                    #print(sentence.to_tree())\n",
    "\n",
    "                    for word in sentence:\n",
    "                        sent.append(word.form)                    # Appending each word in sent list\n",
    "                        if word.form == '।':\n",
    "                            sent2= \" \".join(sent)                   \n",
    "                            sentence.set_meta('Sentence',sent2)   # Adding sentence in as a meta data\n",
    "                            sent = []\n",
    "                            continue\n",
    "\n",
    "\n",
    "                with open(filename, 'w', encoding = 'utf-8') as f:\n",
    "                    file.write(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def stanza_parse(self):\n",
    "        \n",
    "        '''\n",
    "            Function uses Stanza dependency parser  \n",
    "            #   Input file =  hi_1234_6_pos_rules.txt\n",
    "            #   Output file =  hi_1234_output.txt\n",
    "        '''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "            \n",
    "            if filename.endswith(\"_6_pos_rules.txt\"):\n",
    "                filename_in = filename\n",
    "                #filename_out = filename_in.replace(\"_Stanza_SingleScript_6_udpos_rules.txt\", \"_Stanza_SingleScript_7_Stanza_parse.txt\")\n",
    "                #print(filename_out)\n",
    "\n",
    "                doc = CoNLL.conll2doc(filename)\n",
    "\n",
    "                nlp = stanza.Pipeline(lang='hi', processors='depparse', depparse_pretagged=True)\n",
    "\n",
    "                doc1 = nlp(doc)\n",
    "                #print(doc1)\n",
    "\n",
    "                \n",
    "                #creating new folder in current directory to store the output files\n",
    "                directory=self.filepath.replace(\"temp_files\",\"Output_files\") \n",
    "                #print(directory)\n",
    "                \n",
    "                try:\n",
    "                    os.makedirs(directory,exist_ok=True)\n",
    "                except OSError as error:\n",
    "                    pass\n",
    "\n",
    "\n",
    "                filename_out = file.replace(\"_6_pos_rules.txt\", \"_output.txt\")\n",
    "                filename_out2 = os.path.join(directory, filename_out)\n",
    "                #print(filename_out2)\n",
    "                \n",
    "                CoNLL.write_doc2conll(doc1, filename_out2)     #Coverting stanza document to conll format\n",
    "\n",
    "               \n",
    "    \n",
    "    def parser_output(self):\n",
    "        \n",
    "        self.format1()\n",
    "        self.segment()\n",
    "        self.chunks()\n",
    "        self.misc()\n",
    "        self.stanza_pos()\n",
    "        self.stanza_pos_rules()\n",
    "        self.sent_comment()\n",
    "        self.stanza_parse()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7279aa91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 23:00:40 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 23:00:40 INFO: Use device: cpu\n",
      "2022-03-02 23:00:40 INFO: Loading: tokenize\n",
      "2022-03-02 23:00:40 INFO: Loading: pos\n",
      "2022-03-02 23:00:44 INFO: Loading: lemma\n",
      "2022-03-02 23:00:44 INFO: Done loading processors!\n",
      "2022-03-02 23:01:08 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 23:01:08 INFO: Use device: cpu\n",
      "2022-03-02 23:01:08 INFO: Loading: tokenize\n",
      "2022-03-02 23:01:08 INFO: Loading: pos\n",
      "2022-03-02 23:01:10 INFO: Loading: lemma\n",
      "2022-03-02 23:01:11 INFO: Done loading processors!\n",
      "2022-03-02 23:01:37 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 23:01:37 INFO: Use device: cpu\n",
      "2022-03-02 23:01:37 INFO: Loading: tokenize\n",
      "2022-03-02 23:01:37 INFO: Loading: pos\n",
      "2022-03-02 23:01:42 INFO: Loading: lemma\n",
      "2022-03-02 23:01:43 INFO: Done loading processors!\n",
      "2022-03-02 23:01:59 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 23:01:59 INFO: Use device: cpu\n",
      "2022-03-02 23:01:59 INFO: Loading: depparse\n",
      "2022-03-02 23:02:01 INFO: Done loading processors!\n",
      "2022-03-02 23:03:37 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 23:03:37 INFO: Use device: cpu\n",
      "2022-03-02 23:03:37 INFO: Loading: depparse\n",
      "2022-03-02 23:03:44 INFO: Done loading processors!\n",
      "2022-03-02 23:08:54 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 23:08:54 INFO: Use device: cpu\n",
      "2022-03-02 23:08:54 INFO: Loading: depparse\n",
      "2022-03-02 23:09:02 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Giving the path of directory where output of checks is stored\n",
    "\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/SingleScript/temp_files/\"\n",
    "\n",
    "\n",
    "single_script = Parser(path)\n",
    "\n",
    "single_script.parser_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbca0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16c643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
