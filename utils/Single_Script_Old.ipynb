{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a900501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script/ Sp1 overlap count is 178\n",
      "script/ Sp2 overlap count is 174\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "path =\"E:/Cognitive_Science/Sem_III/HSD621_Masters_Project/syntactic_analysis/single_script/\"\n",
    "\n",
    "\n",
    "def checks(path):\n",
    "    #this script checks the .format file for the follwing errors:\n",
    "# double white space\n",
    "# check for pipe symbol instaed of hindi purnaviram symbol and undo if found\n",
    "# pairing of annotation tags- even number of instances in each utterance - if not, print the error with filename, begin_time, utterance\n",
    "# the overlap tag should be equal in number for Sp1 and Sp2 - print the no. of # for Sp1 and Sp2\n",
    "\n",
    "\n",
    "\n",
    "#giving the path of the directory\n",
    "    #path =\"E:/Cognitive_Science/Sem_III/HSD621_Masters_Project/syntactic_analysis/csv_files/\"\n",
    "    dirs = os.listdir(path)\n",
    "\n",
    "    # for every file in the directory\n",
    "    for file in dirs:\n",
    "        filename = path+str(file)\n",
    "\n",
    "        if filename.endswith(\"_0.csv\"):\n",
    "            filename_in = filename\n",
    "            filename_out = filename_in.replace(\"_0.csv\", \"_1_new.txt\")\n",
    "            #print (\"in = \"+ filename_in)\n",
    "            #print (\"out = \"+ filename_out)\n",
    "\n",
    "            fields = []\n",
    "            rows = []\n",
    "            column_value = \"\" #variable for the string in the transcription column\n",
    "            data_value= \"\" #variable for the string in the transcription column as it will appear in the output file\n",
    "            speaker_value = \"\" #variable for th string value of Speaker1 or Speaker2\n",
    "            result_list = []\n",
    "\n",
    "            count_overlap_sp1 = 0\n",
    "            count_overlap_sp2 = 0\n",
    "\n",
    "            with open(filename_in, 'r', encoding=\"utf-8\") as csvfile:\n",
    "                csvreader = csv.reader(csvfile)\n",
    "                reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "                #print (filename_in)\n",
    "\n",
    "                for row in reader:\n",
    "\n",
    "                    begin_time = row[\"Begin Time - hh:mm:ss.ms\"]\n",
    "                    end_time = row[\"End Time - hh:mm:ss.ms\"]\n",
    "                    duration = row[\"Duration - hh:mm:ss.ms\"]\n",
    "                    speaker_1 = row[\"Channel1\"]\n",
    "                    speaker_2 = row[\"Channel2\"]\n",
    "\n",
    "\n",
    "                    # the last above line assigns str in \"data\" column to variable data_value. \n",
    "                    # Also replaces a double white space with a single white space \n",
    "                    # Replaces pipe symbol with hindi purnaviram symbol\n",
    "                    if speaker_1 !=\"\":\n",
    "                        data_value=speaker_1;\n",
    "                    else:\n",
    "                        data_value=speaker_2;\n",
    "\n",
    "                    data_value = str(data_value).replace(\"  \",\" \").replace(\"|\",\"।\")\n",
    "                    #count the number of \\d in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_d = int(data_value.count(r\"\\d\"))\n",
    "                    if (count_d % 2) != 0:   \n",
    "                        print (\"ERROR FOR d: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\r in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_r = int(data_value.count(r\"\\r\"))\n",
    "                    if (count_r % 2) != 0:   \n",
    "                        print (\"ERROR FOR r: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\h in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_h = int(data_value.count(r\"\\h\"))\n",
    "                    if (count_h % 2) != 0:   \n",
    "                        print (\"ERROR FOR h: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\exp in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_exp = int(data_value.count(r\"\\exp\"))\n",
    "                    if (count_exp % 2) != 0:   \n",
    "                        print (\"ERROR FOR exp: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\q in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_q = int(data_value.count(r\"\\q\"))\n",
    "                    if (count_q % 2) != 0:   \n",
    "                        print (\"ERROR FOR q: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of \\c in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_c = int(data_value.count(r\"\\c\"))\n",
    "                    if (count_c % 2) != 0:   \n",
    "                        print (\"ERROR FOR c: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #count the number of # in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                    count_overlap = int(data_value.count(r\"#\"))\n",
    "                    if (count_overlap % 2) != 0:   \n",
    "                        print (\"ERROR FOR overlap: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                    #to check overlaps - additional check - for the 2 speakers, the total number of overlap tags should be the same\n",
    "                    if speaker_1 != \"\":\n",
    "                        count_overlap_sp1 = count_overlap_sp1 + count_overlap\n",
    "                    if speaker_2 != \"\":\n",
    "                        count_overlap_sp2 = count_overlap_sp2 + count_overlap\n",
    "\n",
    "                    if speaker_1 !=\"\":\n",
    "                        speaker_1=data_value;\n",
    "                    else:\n",
    "                        speaker_2=data_value;\n",
    "\n",
    "                    result_list.append([begin_time, end_time, duration, speaker_1, speaker_2])\n",
    "                    #print(result_list)\n",
    "\n",
    "            #the following prints for each file, the total number of overlap tags for Sp1 and Sp2 \n",
    "            print (str(filename_in[-20:-13]) + \" Sp1 overlap count is \" + str(int(count_overlap_sp1)))\n",
    "            print (str(filename_in[-20:-13]) + \" Sp2 overlap count is \" + str(int(count_overlap_sp2)))\n",
    "\n",
    "                # writing to csv file\n",
    "            with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "                # creating a csv writer object\n",
    "                csvwriter = csv.writer(csvfile)\n",
    "                # writing the fields\n",
    "                fields = [\"Begin Time - hh:mm:ss.ms\",\"End Time - hh:mm:ss.ms\",\"Duration - hh:mm:ss.ms\", 'Channel1','Channel2']\n",
    "                csvwriter.writerow(fields)\n",
    "                csvwriter.writerows(result_list)\n",
    "\n",
    "\n",
    "checks(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03543cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import requests\n",
    "import pyconll\n",
    "\n",
    "class Parser:\n",
    "    \n",
    "    def __init__(self, filepath, parser_url):\n",
    "        self.filepath = filepath\n",
    "        self.parser_url= parser_url\n",
    "    def format1(self):\n",
    "        \n",
    "        '''formats the ELAN output to add Speaker_id (Sp1 and Sp2)\n",
    "            merges two Channel columns (for two speakers) to one data column\n",
    "            adds purna viram to the end of every utterance'''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "\n",
    "            if filename.endswith(\"_ss_0checks.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_ss_0checks.txt\",\"_ss_1format1.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                fields = []\n",
    "                rows = []\n",
    "                column_value = \"\" #variable for the string in the transcription column\n",
    "                data_value= \"\" #variable for the string in the transcription column as it will appear in the output file\n",
    "                speaker_value = \"\" #variable for th string value of Speaker1 or Speaker2\n",
    "                result_list = []  \n",
    "\n",
    "                with open(filename_in, 'r', encoding=\"utf-8\") as csvfile:\n",
    "                    csvreader = csv.reader(csvfile)\n",
    "                    reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "                    #print (filename_in)\n",
    "\n",
    "                    for row in reader:\n",
    "\n",
    "                        begin_time = row[\"Begin Time - hh:mm:ss.ms\"]\n",
    "                        end_time = row[\"End Time - hh:mm:ss.ms\"]\n",
    "                        duration = row[\"Duration - hh:mm:ss.ms\"]\n",
    "                        speaker_1 = row[\"Channel1\"]\n",
    "                        speaker_2 = row[\"Channel2\"]\n",
    "                        #print(speaker_1, len(speaker_1), \"-\", speaker_2, len(speaker_2))\n",
    "                        if len(str(speaker_1).strip()) == 0:    # use strip in case there are empty spaces at the end of the string\n",
    "                            column_value = str(speaker_2).strip()    #if empty then this col will be filled with speaker2 value \n",
    "                            speaker_value = \"Sp2\"       # correct speaker id is inserted\n",
    "                        if len(str(speaker_2).strip()) == 0:\n",
    "                            column_value = str(speaker_1).strip()\n",
    "                            speaker_value = \"Sp1\"\n",
    "                        data_value = column_value+str(\" ।\")     #add a period marker at the end of every utterance\n",
    "                        #This last above step can be omitted if the sentence segmentation is taken care of in the transcription\n",
    "                        result_list.append([begin_time, end_time, duration, speaker_value, data_value])\n",
    "                        #print(result_list)\n",
    "\n",
    "                # writing to csv file\n",
    "                with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "                # creating a csv writer object\n",
    "                    csvwriter = csv.writer(csvfile)\n",
    "                # writing the fields\n",
    "                    fields = ['Begin_Time-hh:mm:ss.ms', 'End_Time-hh:mm:ss.ms', 'Duration-hh:mm:ss.ms', 'speaker_id', 'data']\n",
    "                    csvwriter.writerow(fields)\n",
    "                    csvwriter.writerows(result_list)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "    def segment(self):\n",
    "        '''#one word in a row with space as a delimiter (including annotation tags) \n",
    "           #each token/word given an index\n",
    "           #sentence segmentation with purnaviram as delimiter \n",
    "           #sent_id added to each sentence and \n",
    "           #Metadata added as comments before every sentence: begin_time, end_time, duration, speaker_id, contains_overlap\n",
    "           #contains_overlap = True/False (removes # from tokens) \n",
    "           #insert blank row after every sentence (after every PUNCT)'''\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "        \n",
    "            \n",
    "        \n",
    "            if filename.endswith(\"_ss_1format1.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_ss_1format1.txt\", \"_ss_2segment.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                fields = []\n",
    "                rows = []\n",
    "                result_list = []\n",
    "                sent_id = 1\n",
    "\n",
    "                with open(filename_in, 'r',encoding=\"utf-8\") as csvfile:\n",
    "                    csvreader = csv.reader(csvfile)\n",
    "                    reader = csv.DictReader(csvfile, delimiter=',')\n",
    "\n",
    "                    result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "\n",
    "                    for row in reader:\n",
    "                        word_index = 1\n",
    "                        file_name= filename_in[-16:-9]\n",
    "                        begin_time = row[\"Begin_Time-hh:mm:ss.ms\"]\n",
    "                        end_time = row[\"End_Time-hh:mm:ss.ms\"]\n",
    "                        duration = row[\"Duration-hh:mm:ss.ms\"]\n",
    "                        speaker_name = row[\"speaker_id\"]\n",
    "                        rowdata = str(row['data']).strip()\n",
    "                        word_list = rowdata.split(' ')\n",
    "                        overlap_value = rowdata.find(\"#\")\n",
    "                        if overlap_value == -1:\n",
    "                            ol = \"False\"\n",
    "                        else: \n",
    "                            ol = \"True\"\n",
    "\n",
    "                        #print (word_list)\n",
    "\n",
    "                        result_list.append([(\"# begin_time = \" + str(begin_time)), \"\"])\n",
    "                        result_list.append([(\"# end_time = \" + str(end_time)), \"\"])\n",
    "                        result_list.append([(\"# duration = \" + str(duration)), \"\"])\n",
    "                        result_list.append([(\"# speaker_id = \" + str(speaker_name)), \"\"])\n",
    "                        result_list.append([(\"# contains_overlap = \"+ol), \"\"])\n",
    "\n",
    "                        i = 0\n",
    "                        N = len(word_list)\n",
    "\n",
    "                        for word in word_list:\n",
    "                            i = i + 1\n",
    "\n",
    "                            #if word != \"\": #this line creates a check for double spaces - not required now, as double speces eliminated by checks.py\n",
    "                            if word == \"#\":\n",
    "                                continue\n",
    "\n",
    "                            if word == \"।\" and i < N:\n",
    "                                result_list.append([word_index, word])\n",
    "                                result_list.append([\"\", \"\"])\n",
    "\n",
    "                                word_index = 1\n",
    "                                sent_id = sent_id+1        \n",
    "                                result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "\n",
    "                                result_list.append([(\"# begin_time = \" + str(begin_time)),\"\"])\n",
    "                                result_list.append([(\"# end_time = \" + str(end_time)),\"\"])\n",
    "                                result_list.append([(\"# duration = \" + str(duration)),\"\"])\n",
    "                                result_list.append([(\"# speaker_id = \" + str(speaker_name)),\"\"])\n",
    "                                result_list.append([(\"# contains_overlap = \"+ol), \"\"])\n",
    "                            elif word == \"।\":\n",
    "                                result_list.append([word_index, word])\n",
    "                                result_list.append([\"\", \"\"])\n",
    "\n",
    "                                word_index = 1\n",
    "                                sent_id = sent_id+1        \n",
    "                                result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "                            else:\n",
    "                                result_list.append([word_index, word])\n",
    "                                word_index = word_index+1\n",
    "\n",
    "\n",
    "                with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "                    # creating a csv writer object\n",
    "                    csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "                    csvwriter.writerows(result_list)\n",
    "                \n",
    "        \n",
    "    def chunks(self):\n",
    "        '''# creates chunks based on (outermost) annotation tags \n",
    "            # re-orders the word indices within sentences after chunking\n",
    "            # adds prefix and suffix of outermost tag to the chunk \n",
    "            # creates a misc string for outermost tag\n",
    "            # nested tags are included in the chunk as is\n",
    "            # maintain the structure of metadta in comments \n",
    "            '''\n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = path+str(file)\n",
    "\n",
    "            if filename.endswith(\"_ss_2segment.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_ss_2segment.txt\", \"_ss_3chunks.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                result_list= []\n",
    "                sent_id = 1\n",
    "                chunk_list = []\n",
    "                chunk = \"\"\n",
    "                chunk_id = \"\"\n",
    "                word_index = 1\n",
    "                misc = \"\"\n",
    "                with open(filename_in,'r', encoding = 'utf-8') as csvfile:\n",
    "                    csvreader = csv.reader(csvfile, delimiter='\\t')\n",
    "\n",
    "                    for row in csvreader:  \n",
    "                        word_n = row[0]\n",
    "                        #print(word_n)\n",
    "                        word_form = row[1]\n",
    "                        \n",
    "                        #print(word_form)\n",
    "                        #print(type(word_form))\n",
    "                        if len(chunk_list) != 0 and word_form[0]!=\"\\\\\":\n",
    "                            chunk_list.append(word_form)\n",
    "\n",
    "                        if word_n.startswith(\"#\"):\n",
    "                            result_list.append([word_n,\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"])\n",
    "                            continue\n",
    "                        elif word_n == \"\":\n",
    "                            continue \n",
    "                        elif word_form ==\"।\":\n",
    "                            if len(chunk_list) != 0:\n",
    "                                chunk_list.append(chunk_id)\n",
    "                                chunk = \"_\".join(chunk_list)\n",
    "                                #chunk = chunk.replace(\"\\\\\",\"\")\n",
    "                                result_list.append([str(word_index),chunk,misc])\n",
    "                                word_index = word_index + 1\n",
    "                                result_list.append([str(word_index),word_form,\"_\"])\n",
    "                                result_list.append([\"\",\"\",\"\"])\n",
    "                                word_index = 1\n",
    "                                chunk_list =[]  \n",
    "                                #this block should check instances of errors where tag is not closed. This code should serve as a barrier for the error to carry over beyond sentence boundary.\n",
    "                                #check in the end if this block works, by manually removing one of the closing tags (outermost, not nested) in a sentence\n",
    "                            else:\n",
    "                                result_list.append([str(word_index),word_form,\"_\"])\n",
    "                                result_list.append([\"\",\"\",\"\"])\n",
    "                                word_index = 1  \n",
    "\n",
    "                        elif word_form.startswith(\"\\\\\"):\n",
    "                            if len(chunk_list) == 0:\n",
    "                                chunk_id = word_form\n",
    "                                chunk_list.append(word_form)\n",
    "\n",
    "                                misc = word_form[1]+\"=Matrix_Tag\"  #As a dictionary item?\n",
    "                                #print(misc)\n",
    "                            elif word_form == chunk_id:\n",
    "                                chunk_list.append(word_form)\n",
    "                                chunk = \"_\".join(chunk_list)\n",
    "                                #chunk = chunk.replace(\"\\\\\",\"\")\n",
    "                                result_list.append([str(word_index),chunk,misc])\n",
    "                                word_index = word_index +1\n",
    "                                chunk_list =[]\n",
    "                            else:\n",
    "                                chunk_list.append(word_form)\n",
    "                            continue                            \n",
    "\n",
    "                        else:\n",
    "                            if len(chunk_list) == 0:\n",
    "                                result_list.append([str(word_index) ,word_form,\"_\"])\n",
    "                                word_index = word_index + 1\n",
    "\n",
    "                with open(filename_out, 'w',encoding = 'utf-8', newline='') as csvfile:\n",
    "                    csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "                    csvwriter.writerows(result_list)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    def misc(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "        \n",
    "            if filename.endswith(\"_ss_3chunks.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_ss_3chunks.txt\", \"_ss_4misc.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                count=0;\n",
    "                with open(filename_in,'r', encoding = 'utf-8') as csvfile:\n",
    "                    csvreader = csv.reader(csvfile, delimiter='\\t' )\n",
    "\n",
    "                    result_list = []\n",
    "\n",
    "                    for row in csvreader: \n",
    "                        count+=1\n",
    "                        if(count>291):\n",
    "                            break\n",
    "\n",
    "                        word_n = row[0]\n",
    "                        word_form = row[1]\n",
    "                        misc = row[2]\n",
    "                        misc_list= []\n",
    "\n",
    "                        if word_n.startswith(\"#\"):\n",
    "                            result_list.append(word_n+'\\n')\n",
    "                            continue\n",
    "\n",
    "                        elif word_n ==\"\":\n",
    "                            result_list.append('\\n')  \n",
    "                            continue\n",
    "\n",
    "                        elif word_form.startswith(\"\\\\\"):\n",
    "                            misc_list.append(misc)\n",
    "                            if \"_\\\\q_\" in word_form or \"_\\\\r_\" in word_form or \"_\\\\d_\" in word_form or \"_\\\\h_\" in word_form or \"_\\\\c_\" in word_form or \"_\\\\exp_\" in word_form:\n",
    "                                misc_list.append(\"nested_tag=True\")\n",
    "                                if \"_\\\\q_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\q\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"q=True'\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\r_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\r\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"r=True'\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\d_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\d\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"d=True'\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\h_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\h\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"h=True'\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\c_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\c\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"c=True'\"+subtag+\"'\")\n",
    "                                    #print (subtag)\n",
    "                                if \"_\\\\exp_\" in word_form:\n",
    "                                    subtag = word_form.split(\"\\\\exp\")[1].strip(\"_\")\n",
    "                                    misc_list.append(\"exp=True'\"+subtag+\"'\")\n",
    "                                    #print (subtag)   \n",
    "                                misc = r\"|\".join(misc_list) \n",
    "                                #print (type(misc))\n",
    "                                #print(misc_list)\n",
    "                                word_form = word_form.replace(\"\\\\q_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\q\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\r_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\r\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\d_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\d\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\h_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\h\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\c_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\c\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\exp_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\exp\", \"\")\n",
    "                                word_form = word_form.replace(\"__\", \"_\")\n",
    "\n",
    "                                result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+misc+'\\n')\n",
    "                            else:\n",
    "                                word_form = word_form.replace(\"\\\\q_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\q\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\r_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\r\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\d_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\d\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\h_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\h\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\c_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\c\", \"\")\n",
    "                                word_form = word_form.replace(\"\\\\exp_\", \"\")\n",
    "                                word_form = word_form.replace(\"_\\\\exp\", \"\")\n",
    "                                word_form = word_form.replace(\"__\", \"_\")\n",
    "\n",
    "                                result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+misc+'\\n')    \n",
    "                        else:\n",
    "                            result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\n')\n",
    "\n",
    "\n",
    "                with open(filename_out, 'w', encoding = 'utf-8') as f:\n",
    "                        for item in result_list:\n",
    "                            f.write(item)\n",
    "                        \n",
    "    def udpos(self):\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "            \n",
    "            if filename.endswith(\"_ss_4misc.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_ss_4misc.txt\", \"_ss_5udpos.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                with open(filename_in,'rb') as payload:\n",
    "                    #print(payload.read())\n",
    "                    #var=payload.read()\n",
    "                    #print(var)\n",
    "                    parameters = {\n",
    "                        \"data\": payload.read(),\n",
    "                        \"model\": \"hindi\",\n",
    "                        \"input\":\"conllu\",\n",
    "                        \"tagger\": 1,\n",
    "                    }\n",
    "\n",
    "                   #print(payload.read())\n",
    "\n",
    "                    response = requests.post(self.parser_url, parameters)\n",
    "                    #print(response)\n",
    "                    the_output = response.json()\n",
    "\n",
    "                    with open (filename_out, 'w', encoding= \"utf-8\") as file:\n",
    "                        file.write(the_output[\"result\"])\n",
    "                    \n",
    "        \n",
    "    def udpos_rules(self):\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "            \n",
    "            if filename.endswith(\"_ss_5udpos.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_ss_5udpos.txt\", \"_ss_6udpos_rules.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                file = pyconll.load_from_file(filename_in)\n",
    "\n",
    "                for sentence in file:\n",
    "                    for token in sentence:\n",
    "                        misc_item1 = list((token.misc).items())\n",
    "                        #print (misc_item1)\n",
    "                        tag = [('q', {'Matrix_Tag'}), ('r', {'Matrix_Tag'}), ('d', {'Matrix_Tag'}), ('h', {'Matrix_Tag'}),\n",
    "                               ('c', {'Matrix_Tag'}), ('exp', {'Matrix_Tag'})]\n",
    "                        for i in tag:\n",
    "                            if i in misc_item1:\n",
    "                                token.upos =\"X\"\n",
    "                            #print(misc_item1)\n",
    "                            #print(token.upos)\n",
    "                        #help(token.misc)    \n",
    "                        if str(token.form) == \"[laughter]\":\n",
    "                            token.upos = \"X\"    \n",
    "\n",
    "                        if str(token.form) == \"[incomprehensible]\":\n",
    "                            token.upos = \"X\"\n",
    "\n",
    "                        if str(token.form) == \"[noise]\":\n",
    "                            token.upos = \"X\"\n",
    "\n",
    "                        if str(token.form) == \"हाँ\":\n",
    "                            token.upos = \"PART\"        \n",
    "\n",
    "                        if str(token.form) == \"ह्म\":\n",
    "                            token.upos = \"PART\" \n",
    "\n",
    "                        if str(token.form) == \"[anonymized]\":\n",
    "                            token.upos = \"PROPN\"\n",
    "\n",
    "                        if str(token.lemma) == \"है\" and str(token.xpos) == \"VM\":\n",
    "                            token.upos = \"VERB\"\n",
    "\n",
    "                        if str(token.lemma) == \"था\" and str(token.xpos) == \"VM\":\n",
    "                            token.upos = \"VERB\"            \n",
    "\n",
    "                with open(filename_out, 'w', encoding = 'utf-8') as f:\n",
    "                    file.write(f)            \n",
    "\n",
    "                \n",
    "    def udpos_parse(self):\n",
    "        \n",
    "        dirs = os.listdir(self.filepath)\n",
    "        #print(dirs)\n",
    "        for file in dirs:\n",
    "            filename = self.filepath+str(file)\n",
    "            \n",
    "            if filename.endswith(\"_ss_6udpos_rules.txt\"):\n",
    "                filename_in = filename\n",
    "                filename_out = filename_in.replace(\"_ss_6udpos_rules.txt\", \"_ss_7udpos_parse.txt\")\n",
    "                #print (\"in = \"+ filename_in)\n",
    "                #print (\"out = \"+ filename_out)\n",
    "\n",
    "                with open(filename_in,'rb') as payload:\n",
    "                    parameters = {\n",
    "                        \"data\": payload.read(),\n",
    "                        \"model\": \"hindi\",\n",
    "                        \"input\":\"conllu\",\n",
    "                        \"parser\": 1,\n",
    "                    }\n",
    "\n",
    "                    response = requests.post(self.parser_url, parameters)\n",
    "                    the_output = response.json()\n",
    "\n",
    "                    with open (filename_out, 'w', encoding= \"utf-8\") as file:\n",
    "                        file.write(the_output[\"result\"])\n",
    "                    \n",
    "                    \n",
    "    def parser_output(self):\n",
    "        \n",
    "        self.format1()\n",
    "        self.segment()\n",
    "        self.chunks()\n",
    "        self.misc()\n",
    "        self.udpos()\n",
    "        self.udpos_rules()\n",
    "        self.udpos_parse()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1003ab79",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9b7002c92289>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparser_url\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34m\"http://lindat.mff.cuni.cz/services/udpipe/api/process\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(test1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-cd8d8df85d77>\u001b[0m in \u001b[0;36mparser_output\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mudpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-cd8d8df85d77>\u001b[0m in \u001b[0;36mchunks\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m                         \u001b[1;31m#print(word_form)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                         \u001b[1;31m#print(type(word_form))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword_form\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m\"\\\\\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m                             \u001b[0mchunk_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_form\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "path = \"E:/Cognitive_Science/Sem_III/HSD621_Masters_Project/syntactic_analysis/single_script/\"\n",
    "parser_url =  \"http://lindat.mff.cuni.cz/services/udpipe/api/process\"\n",
    "test1=Parser(path, parser_url)\n",
    "test1.parser_output()\n",
    "#print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d55dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095340f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
