{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ec55f1",
   "metadata": {},
   "source": [
    "## 0. Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec6106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "11da09b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peline/ Sp1 overlap count is 146\n",
      "peline/ Sp2 overlap count is 142\n",
      "ERROR FOR overlap: ne/hi_6 00:08:21.849 # ठीक है # वो उस का # हस्बेंड अगर चाहता है जैसे रखना चाहता है वैसे रहे तो ठीक है । # मतलब कम से कम # \\r उस का \\r # माँ की आदत # वादत छोड़ी है मतलब छूट ही जाता है शादी के बाद  ।\n",
      "ERROR FOR overlap: ne/hi_6 00:16:10.331 हाँ । \\r नहीं \\r अब मतलब वहाँ ही रहना ही है । # मकान तुम्हारा है ही । तो # तुम्हें मतलब वहाँ का ले के ही रहना चाहिए # ।\n",
      "ERROR FOR overlap: ne/hi_6 00:18:35.653 कि नहीं । अब \\r 10 1 द \\r 1 हफ़्ते इस इंटर्व्यू के लिए इंतज़ार करना है । 1 हफ़्ते अगले के लिए इंतज़ार करना है । # [laughter] फ़िर अगल ।\n",
      "ERROR FOR overlap: ne/hi_6 00:23:08.956 # यहाँ पे # ये बच्चे हैं ना । तो इन लोगों से ही तुम मतलब # पता कर सकती हो ना कि हम लोग हैं कहाँ ।\n",
      "ERROR FOR r: ne/hi_6 00:25:30.307 \\r तुम क्या कह रही \\r तुम ने कार्ड \\r तुम्हारा आया तो है कार्ड मेरे पास ।\n",
      "peline/ Sp1 overlap count is 388\n",
      "peline/ Sp2 overlap count is 372\n"
     ]
    }
   ],
   "source": [
    "#this script checks the .format file for the follwing errors:\n",
    "# double white space\n",
    "# check for pipe symbol instaed of hindi purnaviram symbol and undo if found\n",
    "# pairing of annotation tags- even number of instances in each utterance - if not, print the error with filename, begin_time, utterance\n",
    "# the overlap tag should be equal in number for Sp1 and Sp2 - print the no. of # for Sp1 and Sp2\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "#giving the path of the directory\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/\"    #Purnaviram/\" #Stanza_Pipeline/\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_0.csv\"):\n",
    "        filename_in = filename\n",
    "        \n",
    "        #filename_out = filename_in.replace(\"_0.csv\", \"_0_checks.txt\")\n",
    "        #print (\"in = \"+ filename_in)\n",
    "        #print (\"out = \"+ filename_out)\n",
    "\n",
    "        fields = []\n",
    "        rows = []\n",
    "        column_value = \"\" #variable for the string in the transcription column\n",
    "        data_value= \"\" #variable for the string in the transcription column as it will appear in the output file\n",
    "        speaker_value = \"\" #variable for th string value of Speaker1 or Speaker2\n",
    "        result_list = []\n",
    "\n",
    "        count_overlap_sp1 = 0\n",
    "        count_overlap_sp2 = 0\n",
    "\n",
    "        with open(filename_in, 'r', encoding=\"utf-8\") as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "            #print (filename_in)\n",
    "            \n",
    "            \n",
    "            for row in reader:\n",
    "                \n",
    "                begin_time = row[\"Begin Time - hh:mm:ss.ms\"]\n",
    "                end_time = row[\"End Time - hh:mm:ss.ms\"]\n",
    "                duration = row[\"Duration - hh:mm:ss.ms\"]\n",
    "                speaker_1 = row[\"Channel1\"]\n",
    "                speaker_2 = row[\"Channel2\"]\n",
    "            \n",
    "                \n",
    "                # the last above line assigns str in \"data\" column to variable data_value. \n",
    "                # Also replaces a double white space with a single white space \n",
    "                # Replaces pipe symbol with hindi purnaviram symbol\n",
    "                if speaker_1 !=\"\":\n",
    "                    data_value=speaker_1+str(\" ।\");     #Adding Purnaviram to see how pipeline works when \n",
    "                else:                                  #transcription already have it   \n",
    "                    data_value=speaker_2+str(\" ।\");\n",
    "                \n",
    "                \n",
    "                \n",
    "                data_value = str(data_value).replace(\"  \",\" \").replace(\"|\",\"।\")\n",
    "                #count the number of \\d in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                count_d = int(data_value.count(r\"\\d\"))    #r is used to indicate \\ as a part of string and not a special character\n",
    "                if (count_d % 2) != 0:   \n",
    "                    print (\"ERROR FOR d: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                #count the number of \\r in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                count_r = int(data_value.count(r\"\\r\"))\n",
    "                if (count_r % 2) != 0:   \n",
    "                    print (\"ERROR FOR r: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                #count the number of \\h in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                count_h = int(data_value.count(r\"\\h\"))\n",
    "                if (count_h % 2) != 0:   \n",
    "                    print (\"ERROR FOR h: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                #count the number of \\exp in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                count_exp = int(data_value.count(r\"\\exp\"))\n",
    "                if (count_exp % 2) != 0:   \n",
    "                    print (\"ERROR FOR exp: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                #count the number of \\q in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                count_q = int(data_value.count(r\"\\q\"))\n",
    "                if (count_q % 2) != 0:   \n",
    "                    print (\"ERROR FOR q: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                #count the number of \\c in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                count_c = int(data_value.count(r\"\\c\"))\n",
    "                if (count_c % 2) != 0:   \n",
    "                    print (\"ERROR FOR c: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                #count the number of # in the utterance and print (filename+begin_time_utterance) if odd number of occurances\n",
    "                count_overlap = int(data_value.count(r\"#\"))\n",
    "                if (count_overlap % 2) != 0:   \n",
    "                    print (\"ERROR FOR overlap: \" + filename_in[-16:-9] + \" \" + begin_time + \" \" + data_value)\n",
    "\n",
    "                #to check overlaps - additional check - for the 2 speakers, the total number of overlap tags should be the same\n",
    "                if speaker_1 != \"\":\n",
    "                    count_overlap_sp1 = count_overlap_sp1 + count_overlap\n",
    "                if speaker_2 != \"\":\n",
    "                    count_overlap_sp2 = count_overlap_sp2 + count_overlap\n",
    "                \n",
    "                \n",
    "                if speaker_1 !=\"\":\n",
    "                    speaker_1=data_value;\n",
    "                else:\n",
    "                    speaker_2=data_value;\n",
    "                \n",
    "                speaker_1 = str(speaker_1).replace(\"  \",\" \").replace(\"|\",\"।\")\n",
    "                speaker_2 = str(speaker_2).replace(\"  \",\" \").replace(\"|\",\"।\")\n",
    "                \n",
    "                \n",
    "                result_list.append([begin_time, end_time, duration, speaker_1, speaker_2])\n",
    "                #print(result_list)\n",
    "\n",
    "        #the following prints for each file, the total number of overlap tags for Sp1 and Sp2 \n",
    "        print (str(filename_in[-20:-13]) + \" Sp1 overlap count is \" + str(int(count_overlap_sp1)))\n",
    "        print (str(filename_in[-20:-13]) + \" Sp2 overlap count is \" + str(int(count_overlap_sp2)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        directory= \"temp\" \n",
    "        new_path=os.path.join(path,directory)\n",
    "#         print(new_path)\n",
    "        try:\n",
    "            os.makedirs(new_path,exist_ok=True)\n",
    "        except OSError as error:\n",
    "            pass\n",
    "\n",
    "\n",
    "        new_filename = file.replace(\"_0.csv\", \"_0_checks.txt\")\n",
    "        filename_out2 = os.path.join(new_path,new_filename)\n",
    "        #print(filename_out2)\n",
    "\n",
    "            # writing to csv file\n",
    "        with open(filename_out2, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "            # creating a csv writer object\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            # writing the fields\n",
    "            fields = [\"Begin Time - hh:mm:ss.ms\",\"End Time - hh:mm:ss.ms\",\"Duration - hh:mm:ss.ms\", 'Channel1','Channel2']\n",
    "            csvwriter.writerow(fields)\n",
    "            csvwriter.writerows(result_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714bdbf0",
   "metadata": {},
   "source": [
    "## 1. Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e99ae716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi_6441_0_checks.txt', 'hi_6523_0_checks.txt']\n"
     ]
    }
   ],
   "source": [
    "# formats the ELAN output to add Speaker_id (Sp1 and Sp2)\n",
    "# merges two Channel columns (for two speakers) to one data column\n",
    "# adds purna viram to the end of every utterance\n",
    "#   Input file = hi_1234_0.txt  \n",
    "#   Output file =  hi_1234_1_format.txt\n",
    "\n",
    "import os, sys\n",
    "import csv\n",
    "\n",
    "\n",
    "#giving the path of the directory\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"       # Purnaviram/temp/\n",
    "dirs = os.listdir(path)\n",
    "print(dirs)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_0_checks.txt\"):\n",
    "        filename_in = filename\n",
    "        filename_out = filename_in.replace(\"_0_checks.txt\",\"_1_Stanza_format.txt\")\n",
    "        #print (\"in = \"+ filename_in)\n",
    "        #print (\"out = \"+ filename_out)\n",
    "\n",
    "        fields = []\n",
    "        rows = []\n",
    "        column_value = \"\" #variable for the string in the transcription column\n",
    "        data_value= \"\" #variable for the string in the transcription column as it will appear in the output file\n",
    "        speaker_value = \"\" #variable for th string value of Speaker1 or Speaker2\n",
    "        result_list = []  \n",
    "        \n",
    "        with open(filename_in, 'r', encoding=\"utf-8\") as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "            #print (filename_in)\n",
    "            flag=False\n",
    "            flag1=False\n",
    "            for row in reader:\n",
    "#                 print(row)\n",
    "                begin_time = row[\"Begin Time - hh:mm:ss.ms\"]\n",
    "                end_time = row[\"End Time - hh:mm:ss.ms\"]\n",
    "                duration = row[\"Duration - hh:mm:ss.ms\"]\n",
    "                speaker_1 = row[\"Channel1\"]\n",
    "                speaker_2 = row[\"Channel2\"]\n",
    "                \n",
    "                \n",
    "#                 print(speaker_1, len(speaker_1), \"-\", speaker_2, len(speaker_2))\n",
    "\n",
    "                if len(str(speaker_1).strip()) == 0:    # use strip in case there are empty spaces at the end of the string\n",
    "                    column_value = str(speaker_2).strip()    #if empty then this col will be filled with speaker2 value \n",
    "#                     print(column_value)\n",
    "                    if \"[b_speaker3]\" in (column_value):      #Remove [b_speaker3] and [e_speaker3]\n",
    "                        flag=True\n",
    "                        speaker_value=\"Sp3\"\n",
    "                   \n",
    "                    elif \"[e_speaker3]\" in (column_value) :\n",
    "                        flag=False\n",
    "                        speaker_value=\"Sp3\"\n",
    "                        \n",
    "                    elif flag==True:\n",
    "                        speaker_value=\"Sp3\"\n",
    "                    else:\n",
    "                        speaker_value = \"Sp2\"       # correct speaker id is inserted\n",
    "                elif len(str(speaker_2).strip()) == 0:\n",
    "                    column_value = str(speaker_1).strip()\n",
    "#                     print(column_value)\n",
    "                    if \"[b_speaker3]\" in (column_value):\n",
    "                        flag1=True\n",
    "                        speaker_value=\"Sp3\"\n",
    "                    elif flag1==True:\n",
    "                        speaker_value=\"Sp3\"\n",
    "                    elif \"[e_speaker3]\" in (column_value) :\n",
    "                        flag1=False\n",
    "                        speaker_value=\"Sp3\"   \n",
    "                    else:\n",
    "                        speaker_value = \"Sp1\" \n",
    "\n",
    "                \n",
    "                if \"[b_speaker3]\" in column_value:\n",
    "                    column_value=column_value.replace(\"[b_speaker3]\",\" \").replace(\"  \",\"\")\n",
    "                    \n",
    "                elif \"[e_speaker3]\" in column_value:\n",
    "                    column_value=column_value.replace(\"[e_speaker3]\",\"\").replace(\"  \",\" \")\n",
    "                   \n",
    "                    \n",
    "                data_value = column_value\n",
    "#                 print(data_value)\n",
    "                \n",
    "                result_list.append([begin_time, end_time, duration, speaker_value, data_value])\n",
    "#                 print(result_list)\n",
    "    \n",
    "        # writing to csv file\n",
    "        with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "        # creating a csv writer object\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "        # writing the fields\n",
    "            fields = ['Begin_Time-hh:mm:ss.ms', 'End_Time-hh:mm:ss.ms', 'Duration-hh:mm:ss.ms', 'speaker_id', 'data']\n",
    "            csvwriter.writerow(fields)\n",
    "            csvwriter.writerows(result_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93396007",
   "metadata": {},
   "source": [
    "## 2. Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94addf7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae76fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0c842edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one word in a row with space as a delimiter (including annotation tags)\n",
    "# each token/word given an index\n",
    "# sentence segmentation with purnaviram as delimiter\n",
    "# sent_id added to each sentence\n",
    "# Metadata added as comments before every sentence: begin_time, end_time, duration, speaker_id, contains_overlap\n",
    "# contains_overlap = True/False (removes # from tokens)\n",
    "# insert blank row after every sentence (after every PUNCT)\n",
    "\n",
    "import os, sys\n",
    "import csv\n",
    "\n",
    "#giving the path of the directory\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_1_Stanza_format.txt\"):\n",
    "        filename_in = filename\n",
    "        filename_out = filename_in.replace(\"_1_Stanza_format.txt\", \"_2_Stanza_segments.txt\")\n",
    "        #print (\"in = \"+ filename_in)\n",
    "        #print (\"out = \"+ filename_out)\n",
    "\n",
    "        fields = []\n",
    "        rows = []\n",
    "        result_list = []\n",
    "        sent_id = 1\n",
    "        \n",
    "        with open(filename_in, 'r',encoding=\"utf-8\") as csvfile:\n",
    "            csvreader = csv.reader(csvfile)\n",
    "            reader = csv.DictReader(csvfile, delimiter=',')\n",
    "            \n",
    "#             result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "#             print(result_list)\n",
    "            \n",
    "            for row in reader:\n",
    "                word_index = 1\n",
    "                file_name= filename_in[-16:-9]\n",
    "                begin_time = row[\"Begin_Time-hh:mm:ss.ms\"]\n",
    "                end_time = row[\"End_Time-hh:mm:ss.ms\"]\n",
    "                duration = row[\"Duration-hh:mm:ss.ms\"]\n",
    "                speaker_name = row[\"speaker_id\"]\n",
    "                rowdata = str(row['data']).strip()\n",
    "                word_list = rowdata.split(' ')\n",
    "#                 print(word_list)\n",
    "  \n",
    "                flag = False\n",
    "                var= False\n",
    "\n",
    "                count = 0\n",
    "                count_list = [\"#\", ]\n",
    "                N = len(word_list)\n",
    "                temp_list = []\n",
    "#                 print(temp_list)\n",
    "                for word in word_list:\n",
    "#                     if not word.startswith((\"#\", \"\\\\\",)):\n",
    "#                         print(\"###\")                   \n",
    "#                         count+=1\n",
    "#                     print(word)\n",
    "                    #if word != \"\": #this line creates a check for double spaces - not required now, as double speces eliminated by checks.py\n",
    "                    if word == \"#\":\n",
    "                        if flag == False:\n",
    "                            flag = True     \n",
    "                        else:\n",
    "                            flag = False\n",
    "                            var = True\n",
    "                        \n",
    "                \n",
    "                    elif word == \"।\":\n",
    "                        temp_list.append([word_index, word])\n",
    "                            \n",
    "                        result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "                        result_list.append([(\"# begin_time = \" + str(begin_time)), \"\"])\n",
    "                        result_list.append([(\"# end_time = \" + str(end_time)), \"\"])\n",
    "                        result_list.append([(\"# duration = \" + str(duration)), \"\"])\n",
    "                        result_list.append([(\"# speaker_id = \" + str(speaker_name)), \"\"])\n",
    "                        if flag ==True:\n",
    "                            x= True\n",
    "                        elif var ==True:\n",
    "                            x= True\n",
    "                            var = False\n",
    "                        else:\n",
    "                            x=False\n",
    "                        result_list.append([(\"# contains_overlap = \"+str(x)), \"\"])\n",
    "                        result_list.extend(temp_list)\n",
    "                        result_list.append([\"\", \"\"])\n",
    "                        \n",
    "                        temp_list = []\n",
    "                        word_index = 1\n",
    "                        sent_id = sent_id+1        \n",
    "#                         if flag == True:\n",
    "#                             ol= True\n",
    "#                         else:\n",
    "#                             ol = False\n",
    "                    \n",
    "                    else:\n",
    "                        temp_list.append([word_index, word])\n",
    "#                         print(temp_list)\n",
    "                        word_index = word_index+1\n",
    "          \n",
    "\n",
    "        with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "            # creating a csv writer object\n",
    "            csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "            csvwriter.writerows(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647915f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c6cd4e8",
   "metadata": {},
   "source": [
    "## 3. Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0c8b367b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#3_CHUNKS.PY\n",
    "\n",
    "# creates chunks based on (outermost) annotation tags \n",
    "# re-orders the word indices within sentences after chunking\n",
    "# adds prefix and suffix of outermost tag to the chunk \n",
    "# creates a misc string for outermost tag\n",
    "# nested tags are included in the chunk as is\n",
    "# maintain the structure of metadta in comments \n",
    "\n",
    "import os\n",
    "import csv \n",
    "\n",
    "\n",
    "#giving the path of the directory\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_2_Stanza_segments.txt\"):\n",
    "        filename_in = filename\n",
    "        filename_out = filename_in.replace(\"_2_Stanza_segments.txt\", \"_3_Stanza_chunks.txt\")\n",
    "        #print (\"in = \"+ filename_in)\n",
    "        #print (\"out = \"+ filename_out)\n",
    "\n",
    "        result_list= []\n",
    "        sent_id = 1\n",
    "        chunk_list = []\n",
    "        chunk = \"\"\n",
    "        chunk_id = \"\"\n",
    "        word_index = 1\n",
    "        misc = {}\n",
    "        print(type(misc))\n",
    "        with open(filename_in,'r', encoding = 'utf-8') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter='\\t')\n",
    "\n",
    "            for row in csvreader:  \n",
    "                word_n = row[0]\n",
    "#                 print(word_n)\n",
    "                word_form = row[1]\n",
    "#                 print(word_form)\n",
    "                #print(type(word_form))\n",
    "                if len(chunk_list) != 0 and word_form[0]!=\"\\\\\":\n",
    "                    chunk_list.append(word_form)\n",
    "\n",
    "                if word_n.startswith(\"#\"):\n",
    "                    result_list.append([word_n,\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"])\n",
    "                    continue\n",
    "                elif word_n == \"\":\n",
    "                    continue \n",
    "                elif word_form ==\"।\":\n",
    "                    if len(chunk_list) != 0:\n",
    "                        chunk_list.append(chunk_id)\n",
    "                        chunk = \"_\".join(chunk_list)\n",
    "                        #chunk = chunk.replace(\"\\\\\",\"\")\n",
    "                        result_list.append([str(word_index),chunk,misc])\n",
    "                        word_index = word_index + 1\n",
    "                        result_list.append([str(word_index),word_form,\"_\"])\n",
    "                        result_list.append([\"\",\"\",\"\"])\n",
    "                        word_index = 1\n",
    "                        chunk_list =[]  \n",
    "                        #this block should check instances of errors where tag is not closed. This code should serve as a barrier for the error to carry over beyond sentence boundary.\n",
    "                        #check in the end if this block works, by manually removing one of the closing tags (outermost, not nested) in a sentence\n",
    "                    else:\n",
    "                        result_list.append([str(word_index),word_form,\"_\"])\n",
    "                        result_list.append([\"\",\"\",\"\"])\n",
    "                        word_index = 1  \n",
    "\n",
    "                elif word_form.startswith(\"\\\\\"):\n",
    "#                     print(word_form)\n",
    "                    if len(chunk_list) == 0:\n",
    "                        chunk_id = word_form\n",
    "                        chunk_list.append(word_form)\n",
    "                        \n",
    "#                         print(word_form[1])\n",
    "                        \n",
    "                        if word_form[1]=='q':\n",
    "                            misc = \"Quote=Matrix_Tag\"\n",
    "                        if word_form[1]=='c': \n",
    "                            misc = \"CodeSwitch=Matrix_Tag\"\n",
    "                        if word_form[1]=='r':\n",
    "                            misc = \"Repair=Matrix_Tag\"\n",
    "                        if word_form[1]=='d':\n",
    "                            misc = \"Disfluency=Matrix_Tag\"\n",
    "                        if word_form[1]=='h':\n",
    "                            misc = \"Hesitation=Matrix_Tag\"\n",
    "                        if word_form[1]=='e':\n",
    "                            misc = \"Expletive=Matrix_Tag\"\n",
    "                        \n",
    "#                         print(misc)\n",
    "                    elif word_form == chunk_id:\n",
    "                        chunk_list.append(word_form)\n",
    "#                         print(chunk_list)\n",
    "                        chunk = \"_\".join(chunk_list)\n",
    "#                         print(chunk)\n",
    "                        #chunk = chunk.replace(\"\\\\\",\"\")\n",
    "                        result_list.append([str(word_index),chunk,misc])\n",
    "                        word_index = word_index +1\n",
    "                        chunk_list =[]\n",
    "                    else:\n",
    "                        chunk_list.append(word_form)\n",
    "                    continue                            \n",
    "                    \n",
    "                else:\n",
    "                    if len(chunk_list) == 0:\n",
    "                        result_list.append([str(word_index) ,word_form,\"_\"])\n",
    "                        word_index = word_index + 1\n",
    "                    \n",
    "        with open(filename_out, 'w',encoding = 'utf-8', newline='') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "            csvwriter.writerows(result_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad2a7d",
   "metadata": {},
   "source": [
    "## 4. Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cccf8e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintains the sent_id and word_index of the tokens\n",
    "# for each nested tag, adds information to misc column\n",
    "# removes tags from the chunks\n",
    "# format of chunk: prefix & suffix \"_\" retained (is this needed?)\n",
    "# formats the output to conllu format \n",
    "# maintains the structure of metadata in comments\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "#giving the path of the directory\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_3_Stanza_chunks.txt\"):\n",
    "        filename_in = filename\n",
    "        filename_out = filename_in.replace(\"_3_Stanza_chunks.txt\", \"_4_Stanza_misc.txt\")\n",
    "        #print (\"in = \"+ filename_in)\n",
    "        #print (\"out = \"+ filename_out)\n",
    "        \n",
    "        \n",
    "        with open(filename_in,'r', encoding = 'utf-8') as csvfile:\n",
    "            csvreader = csv.reader(csvfile, delimiter='\\t' )\n",
    "            \n",
    "            result_list = []\n",
    "               \n",
    "            sent1 = []\n",
    "            #print(sent1)\n",
    "            sent2=[]\n",
    "            for row in csvreader:\n",
    "                #print(row)\n",
    "                word_n = row[0]\n",
    "                #print(type(word_n))\n",
    "#                 print(word_n)\n",
    "                word_form = row[1]\n",
    "#                 print(word_form)\n",
    "                misc = row[2]\n",
    "#                 print(misc)\n",
    "                misc_list= []\n",
    "                \n",
    "               \n",
    "                \n",
    "                #result_list.append(\"#\")\n",
    "                if word_n.startswith(\"#\"):\n",
    "                    result_list.append(word_n+'\\n')\n",
    "\n",
    "                    continue\n",
    "                    \n",
    "               \n",
    "                if word_n.startswith(\"# contains_overlap\"):      #Why this step?\n",
    "                    print(\"#######\")\n",
    "                    result_list.append('\\n'+\"#\")\n",
    "                        \n",
    "\n",
    "                elif word_n ==\"\":\n",
    "                    result_list.append('\\n')  \n",
    "                    continue\n",
    "                \n",
    "                elif word_form.startswith(\"\\\\\"):\n",
    "                    misc_list.append(misc)\n",
    "                    #print(misc)\n",
    "                    if \"_\\\\q_\" in word_form or \"_\\\\r_\" in word_form or \"_\\\\d_\" in word_form or \"_\\\\h_\" in word_form or \"_\\\\c_\" in word_form or \"_\\\\exp_\" in word_form:\n",
    "                        misc_list.append(\"NestedTag=True\")\n",
    "                        #misc_list.append({\"NestedTag\":\"True\"})\n",
    "                        if \"_\\\\q_\" in word_form:\n",
    "                            subtag = word_form.split(\"\\\\q\")[1].strip(\"_\")\n",
    "                            misc_list.append(\"Quote='\"+subtag+\"'\")\n",
    "                            #print (subtag)\n",
    "                        if \"_\\\\r_\" in word_form:\n",
    "                            subtag = word_form.split(\"\\\\r\")[1].strip(\"_\")\n",
    "                            misc_list.append(\"Repair='\"+subtag+\"'\")\n",
    "                            #print (subtag)\n",
    "                        if \"_\\\\d_\" in word_form:\n",
    "                            subtag = word_form.split(\"\\\\d\")[1].strip(\"_\")\n",
    "                            misc_list.append(\"Disfluency='\"+subtag+\"'\")\n",
    "                            #print (subtag)\n",
    "                        if \"_\\\\h_\" in word_form:\n",
    "                            subtag = word_form.split(\"\\\\h\")[1].strip(\"_\")\n",
    "                            misc_list.append(\"Hesitation='\"+subtag+\"'\")\n",
    "                            #print (subtag)\n",
    "                        if \"_\\\\c_\" in word_form:\n",
    "                            subtag = word_form.split(\"\\\\c\")[1].strip(\"_\")\n",
    "                            misc_list.append(\"CodeSwitch='\"+subtag+\"'\")\n",
    "                            #print (subtag)\n",
    "                        if \"_\\\\exp_\" in word_form:\n",
    "                            subtag = word_form.split(\"\\\\exp\")[1].strip(\"_\")\n",
    "                            misc_list.append(\"Expletive='\"+subtag+\"'\")\n",
    "                            #print (subtag)   \n",
    "                        misc = r\"|\".join(misc_list) \n",
    "                        #print (type(misc))\n",
    "                        #print(misc_list)\n",
    "                        #print(misc)\n",
    "                        word_form = word_form.replace(\"\\\\q_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\q\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\r_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\r\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\d_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\d\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\h_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\h\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\c_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\c\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\exp_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\exp\", \"\")\n",
    "                        word_form = word_form.replace(\"__\", \"_\")\n",
    "                        \n",
    "                       \n",
    "                        \n",
    "                        result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+misc+'\\n')\n",
    "                    else:\n",
    "                        word_form = word_form.replace(\"\\\\q_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\q\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\r_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\r\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\d_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\d\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\h_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\h\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\c_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\c\", \"\")\n",
    "                        word_form = word_form.replace(\"\\\\exp_\", \"\")\n",
    "                        word_form = word_form.replace(\"_\\\\exp\", \"\")\n",
    "                        word_form = word_form.replace(\"__\", \"_\")\n",
    "                        \n",
    "                     \n",
    "                        \n",
    "                        result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+misc+'\\n')    \n",
    "                else:\n",
    "                    #word_n.append(\"#\")\n",
    "                    #result_list.append(\"####\"+'\\n')\n",
    "                    result_list.append(word_n+'\\t'+word_form+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\t'+\"_\"+'\\n')\n",
    "                \n",
    "                    #print(result_list)\n",
    "            \n",
    "            \n",
    "        with open(filename_out, 'w', encoding = 'utf-8') as f:\n",
    "                for item in result_list:\n",
    "                    f.write(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb2760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c0e7db1",
   "metadata": {},
   "source": [
    "## 5. Stanza pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2d9d8c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 21:57:26 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 21:57:26 INFO: Use device: cpu\n",
      "2022-03-02 21:57:26 INFO: Loading: tokenize\n",
      "2022-03-02 21:57:26 INFO: Loading: pos\n",
      "2022-03-02 21:57:32 INFO: Loading: lemma\n",
      "2022-03-02 21:57:33 INFO: Done loading processors!\n",
      "2022-03-02 21:58:16 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | hdtb    |\n",
      "| pos       | hdtb    |\n",
      "| lemma     | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 21:58:16 INFO: Use device: cpu\n",
      "2022-03-02 21:58:16 INFO: Loading: tokenize\n",
      "2022-03-02 21:58:16 INFO: Loading: pos\n",
      "2022-03-02 21:58:18 INFO: Loading: lemma\n",
      "2022-03-02 21:58:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# maintains the sent_id and word_index of the tokens\n",
    "# for each nested tag, adds information to misc column\n",
    "# removes tags from the chunks\n",
    "# format of chunk: prefix & suffix \"_\" retained (is this needed?)\n",
    "# formats the output to conllu format \n",
    "# maintains the structure of metadata in comments\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.utils.conll import CoNLL\n",
    "\n",
    "\n",
    "#giving the path of the directory\n",
    "path =  \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_4_Stanza_misc.txt\"):\n",
    "        filename_in = filename\n",
    "        filename_out = filename_in.replace(\"_4_Stanza_misc.txt\", \"_5_Stanza_pos.txt\")\n",
    "        #print (\"in = \"+ filename_in)\n",
    "        #print (\"out = \"+ filename_out)\n",
    "        \n",
    "        \n",
    "        doc = CoNLL.conll2doc(filename_in)   #Coverting txt file to stanza Document\n",
    "        #print(type(doc))\n",
    "    \n",
    "        pos_pipeline = stanza.Pipeline(lang='hi', processors='tokenize,lemma, pos', tokenize_pretokenized=True)\n",
    "\n",
    "        pos_tagged_doc = pos_pipeline(doc)\n",
    "        # print(pos_tagged_doc)\n",
    "\n",
    "        CoNLL.write_doc2conll(pos_tagged_doc, filename_out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb42c2c",
   "metadata": {},
   "source": [
    "## 6. Stanza_pos Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5ec6e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deterministic rules:\n",
    "#   1. The following are tagged X in one step\n",
    "#   * disfluency - prefix \"d_\"\n",
    "#   * repair - prefix \"r_\"\n",
    "#   * hesitation - prefix \"h_\"\n",
    "#   * quote - prefix \"q_\"\n",
    "#   * code_switching - prefix \"c_\"\n",
    "#   * [laughter]\n",
    "#   * [noise]\n",
    "#   * [incomprehensible]\n",
    "#   2. The following tags are modified, unless already tagged X in the previous step:\n",
    "#   * हाँ > PART\n",
    "#   * ह्म > PART\n",
    "#   * [anonymized] >PROPN\n",
    "\n",
    "import os\n",
    "import pyconll\n",
    "\n",
    "#giving the path of the directory\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_5_Stanza_pos.txt\"):\n",
    "        filename_in = filename\n",
    "        filename_out = filename_in.replace(\"_5_Stanza_pos.txt\", \"_6_Stanza_pos_rules.txt\")\n",
    "        #print (\"in = \"+ filename_in)\n",
    "        #print (\"out = \"+ filename_out)\n",
    "\n",
    "        file = pyconll.load_from_file(filename_in)\n",
    "\n",
    "        for sentence in file:\n",
    "            for token in sentence:\n",
    "                #print(token.feats)\n",
    "                misc_item1 = list((token.misc).items())\n",
    "                #print((token.misc.items()))\n",
    "                tag = [('Quote', {'Matrix_Tag'}), ('Repair', {'Matrix_Tag'}), ('Disfluency', {'Matrix_Tag'}), \n",
    "                       ('Hesitation', {'Matrix_Tag'}),('CodeSwitch', {'Matrix_Tag'}), ('Expletive', {'Matrix_Tag'})]\n",
    "                for i in tag:\n",
    "                    if i in misc_item1:\n",
    "                        token.upos =\"X\"\n",
    "                    #print(misc_item1)\n",
    "                    #print(token.upos)\n",
    "                #help(token.misc)    \n",
    "                \n",
    "                if str(token.form) == \"[pause]\":         # ADDED FOR A_ASIDE AND E_ASIDE\n",
    "                    token.upos = \"X\"\n",
    "                \n",
    "                if str(token.form) == \"[aside]\":\n",
    "                    token.upos = \"X\"\n",
    "                    \n",
    "                if str(token.form) == \"[b_aside]\":\n",
    "                    token.upos = \"X\"\n",
    "                    \n",
    "                if str(token.form) == \"[e_aside]\":\n",
    "                    token.upos = \"X\"\n",
    "                \n",
    "                if str(token.form) == \"[laughter]\":\n",
    "                    token.upos = \"X\"\n",
    "                \n",
    "                if str(token.form) == \"[incomprehensible]\":\n",
    "                    token.upos = \"X\"\n",
    "\n",
    "                if str(token.form) == \"[noise]\":\n",
    "                    token.upos = \"X\"\n",
    "\n",
    "                if str(token.form) == \"हाँ\":    #Changed from PART to INTJ\n",
    "                    token.upos = \"INTJ\"        \n",
    "            \n",
    "                if str(token.form) == \"ह्म\":\n",
    "                    token.upos = \"PART\" \n",
    "\n",
    "                if str(token.form) == \"[anonymized]\":\n",
    "                    token.upos = \"PROPN\"\n",
    "\n",
    "#                 if str(token.lemma) == \"है\" and str(token.xpos) == \"VM\":     #Undo this rule\n",
    "#                     token.upos = \"VERB\"\n",
    "\n",
    "#                 if str(token.lemma) == \"था\" and str(token.xpos) == \"VM\":    #Undo this rule\n",
    "#                     token.upos = \"VERB\"            \n",
    "\n",
    "        with open(filename_out, 'w', encoding = 'utf-8') as f:\n",
    "            file.write(f)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ebb70",
   "metadata": {},
   "source": [
    "## 7. Stanza_Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d0d5952f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 20:27:38 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 20:27:38 INFO: Use device: cpu\n",
      "2022-03-02 20:27:38 INFO: Loading: depparse\n",
      "2022-03-02 20:27:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/Output/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 20:32:47 INFO: Loading these models for language: hi (Hindi):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| depparse  | hdtb    |\n",
      "=======================\n",
      "\n",
      "2022-03-02 20:32:47 INFO: Use device: cpu\n",
      "2022-03-02 20:32:47 INFO: Loading: depparse\n",
      "2022-03-02 20:32:53 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/Output/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.utils.conll import CoNLL\n",
    "\n",
    "\n",
    "\n",
    "#giving the path of the directory\n",
    "path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "dirs = os.listdir(path)\n",
    "#print(dirs)\n",
    "\n",
    "# for every file in the directory\n",
    "for file in dirs:\n",
    "    filename = path+str(file)\n",
    "\n",
    "    if filename.endswith(\"_6_Stanza_pos_rules.txt\"):\n",
    "        filename_in = filename\n",
    "        #filename_out = filename_in.replace(\"_6_Stanza_pos_rules.txt\", \"_7_Stanza_parse.txt\")\n",
    "        #print(filename_out)\n",
    "        \n",
    "        doc = CoNLL.conll2doc(filename)\n",
    "        \n",
    "        \n",
    "        nlp = stanza.Pipeline(lang='hi', processors='depparse', depparse_pretagged=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        doc1 = nlp(doc)\n",
    "        #print(doc1)\n",
    "        \n",
    "        \n",
    "        #directory1=filename_in.replace(file,\"\")\n",
    "        directory=path.replace(\"temp\",\"Output\") #It has become static now!!\n",
    "        \n",
    "        #new_path=os.path.join(path,directory)\n",
    "        #print((filename_out[114:]))\n",
    "        print(directory)\n",
    "        try:\n",
    "            os.makedirs(directory,exist_ok=True)\n",
    "        except OSError as error:\n",
    "            pass\n",
    "\n",
    "\n",
    "        new_filename = file.replace(\"_6_Stanza_pos_rules.txt\", \"_7_Stanza_parse.txt\")\n",
    "        filename_out2 = os.path.join(directory,new_filename)\n",
    "        \n",
    "#         filename_out2 = new_path.replace(\"_6_Stanza_pos_rules.txt\", \"_7_Stanza_parse.txt\")\n",
    "#         print(filename_out2)\n",
    "        \n",
    "        \n",
    "        CoNLL.write_doc2conll(doc1, filename_out2)\n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc8128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee120e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef97539b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e3b021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dd47f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pyconll "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32aa4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Sentence Insertion as comment\n",
    "\n",
    "# import os\n",
    "# import pyconll\n",
    "\n",
    "# #giving the path of the directory\n",
    "# path = \"E:/Cognitive_Science/Sem_III/HSD621_Masters_Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline_SingleScript/Output_files/\"\n",
    "# dirs = os.listdir(path)\n",
    "\n",
    "# # for every file in the directory\n",
    "# for file in dirs:\n",
    "#     filename = path+str(file)\n",
    "\n",
    "#     if filename.endswith(\"hi_1385_output - Copy.txt\"):\n",
    "# #         filename_in = filename\n",
    "# #         filename_out = filename_in.replace(\"_6_Stanza_pos_rules.txt\", \"_7_Stanza_parse.txt\")\n",
    "#         #print (\"in = \"+ filename_in)\n",
    "#         #print (\"out = \"+ filename_out)\n",
    "\n",
    "#         file = pyconll.load_from_file(filename)\n",
    "       \n",
    "#         sent = []\n",
    "       \n",
    "#         for sentence in file:\n",
    "#             #print(sentence.to_tree())\n",
    "            \n",
    "#             for word in sentence:\n",
    "#                 sent.append(word.form)\n",
    "#                 if word.form == '।':\n",
    "#                     sent2= \" \".join(sent)\n",
    "#                     sentence.set_meta('Sentence',sent2)\n",
    "#                     sent = []\n",
    "#                     continue\n",
    "\n",
    "        \n",
    "#         with open(filename, 'w', encoding = 'utf-8') as f:\n",
    "#             file.write(f)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b1228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aff24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                #'Gender', {'Masc'}\n",
    "                \n",
    "                #list_token_keys = [list(token.feats.keys()), list(token.deps.keys()), list(token.misc.keys())]\n",
    "                #list_token_values = [token.feats.values(),  token.deps.values(), token.misc.values()]\n",
    "                \n",
    "#                 for i in token.feats.items():\n",
    "#                     print(i)\n",
    "#                     if val in i:\n",
    "\n",
    "#                         count+=1\n",
    "#                         sent_list.append((sentence.meta_value('sent_id'), token.id))\n",
    "                    \n",
    "                    #print(\"####\")\n",
    "                  \n",
    "                    \n",
    "#                     if input_value in token.feats.values():\n",
    "#                         count+=1\n",
    "#                         sent_list.append((sentence.meta_value('sent_id'), token.id))\n",
    "                    #print(\"#####\")\n",
    "#                 \n",
    "#                 print(list_token_keys)\n",
    "                #print(list_token_values)\n",
    "                \n",
    "                #f val in \n",
    "#                 for i in list_token_keys:\n",
    "                \n",
    "#                     if val in i:\n",
    "#                         #count+=1\n",
    "#                         #sent_list.append((sentence.meta_value('sent_id'), token.id))\n",
    "#                         #continue\n",
    "                        \n",
    "#                         input_val = input(\"Enter its value : \")\n",
    "                        \n",
    "#                         for i in list_token_values:\n",
    "                            \n",
    "#                             if input_val in i:\n",
    "#                                 count+=1\n",
    "#                                 print(\"Values found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96286f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Searching certain features\n",
    "\n",
    "\n",
    "# import os\n",
    "# import pyconll\n",
    "\n",
    "# #giving the path of the directory\n",
    "# path = \"E:/Cognitive_Science/Sem_III/HSD621_Masters_Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline_SingleScript/Output_files/\"\n",
    "# dirs = os.listdir(path)\n",
    "\n",
    "# # for every file in the directory\n",
    "# for file in dirs:\n",
    "#     filename = path+str(file)\n",
    "\n",
    "#     if filename.endswith(\"hi_1385_Stanza_SingleScript_7_Stanza_parse.txt\"):\n",
    "#         filename_in = filename\n",
    "        \n",
    "#         #print(\"Instructions: \\n 1. If feats/deps/misc enter input in format \"('Gender', {'Masc'})\". \\n 2. For other enter string\")\n",
    "#         val1 = input('Enter a head : ')\n",
    "#         val2 = input('Enter a deprel : ')\n",
    "#         val3 = input('Enter a UPOS : ')\n",
    "        \n",
    "#         file = pyconll.load_from_file(filename_in)\n",
    "#         count = 0\n",
    "#         sent_list=[]\n",
    "#         for sentence in file:\n",
    "#             for token in sentence:\n",
    "#                 #print((token.feats.items()))\n",
    "                \n",
    "                \n",
    "#                 list_token = [token.form, token.lemma, token.upos, token.xpos, token.head, token.deprel]\n",
    "                      \n",
    "#                 if (val1 and val2 and val3) in list_token:\n",
    "#                     count+=1\n",
    "#                     sent_list.append(sentence.meta_value('Sentence'))   #(sentence.meta_value('sent_id'), token.id)\n",
    "                 \n",
    "                \n",
    "                \n",
    "\n",
    "                    \n",
    "#         print(\"Total number of instances matched are: \", count)\n",
    "#         print(\"\\n\")\n",
    "#         if count!=0:\n",
    "            \n",
    "#             SentId_val= input(\"Do you want to print the list of sentences (YES/NO): \").lower()\n",
    "#             print(\"\\n\")\n",
    "\n",
    "#             if(SentId_val=='yes'):\n",
    "#                 print(\"List of Sentences: \", sent_list)\n",
    "#             else:\n",
    "#                 print(\"Search Completed!!\")\n",
    "#         else:\n",
    "#             pass\n",
    "                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3e45a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2bd2965b6208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cc' is not defined"
     ]
    }
   ],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee53289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import pyconll\n",
    "\n",
    "\n",
    "\n",
    "# #giving the path of the directory\n",
    "# path = \"E:/Cognitive_Science/Sem_III/HSD621_Masters_Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/\"\n",
    "# dirs = os.listdir(path)\n",
    "\n",
    "# # for every file in the directory\n",
    "# for file in dirs:\n",
    "#     filename = path+str(file)\n",
    "\n",
    "#     if filename.endswith(\"_4_Stanza_misc.txt\"):\n",
    "#         filename_in = filename\n",
    "#         filename_out = filename_in.replace(\"_4_Stanza_misc.txt\", \"_4_Stanza_misc_Short.txt\")\n",
    "#         #print (\"in = \"+ filename_in)\n",
    "#         #print (\"out = \"+ filename_out)\n",
    "        \n",
    "#         file = pyconll.load_from_file(filename_in)\n",
    "#         sent = []\n",
    "#         sent2= []\n",
    "       \n",
    "#         for sentence in file:\n",
    "#             #print(sentence.meta_value('contains_overlap'))  \n",
    "#             for word in sentence:\n",
    "#                 sent.append(word.form)\n",
    "#                 if word.form == '।':\n",
    "#                     sent3= \" \".join(sent)\n",
    "#                     sent2.append(\"# sentence =  \"+sent3)\n",
    "#                     sent = []\n",
    "#                     continue\n",
    "#         #print(sent2)\n",
    "#         #print(\"###########\")\n",
    "\n",
    "#         with open(filename_in,'r', encoding = 'utf-8') as csvfile:\n",
    "#             csvreader = csv.reader(csvfile, delimiter='\\t' )\n",
    "            \n",
    "#             result_list = []\n",
    "\n",
    "#             for row in csvreader:\n",
    "#                 #print(row)\n",
    "                \n",
    "#                 result_list.append((row))\n",
    "                \n",
    "#                 if((len(row)!=0) and ('contains_overlap' in row[0])):\n",
    "#                     result_list.append(i in sent2)\n",
    "                \n",
    "#                 #row[0].append()\n",
    "#                 #word_n = row[0]\n",
    "#                 #print(word_n)\n",
    "#                 #word_form = row[1]\n",
    "#                 #print(word_form)\n",
    "#                 #misc = row[2]\n",
    "#                 #print(misc)\n",
    "#                 #misc_list= []\n",
    "#                 #print(result_list)\n",
    "                \n",
    "# #         with open(filename_out, 'w', encoding = 'utf-8') as f:\n",
    "# #             for item in result_list:\n",
    "# #                 f.write(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one word in a row with space as a delimiter (including annotation tags)\n",
    "# # each token/word given an index\n",
    "# # sentence segmentation with purnaviram as delimiter\n",
    "# # sent_id added to each sentence\n",
    "# # Metadata added as comments before every sentence: begin_time, end_time, duration, speaker_id, contains_overlap\n",
    "# # contains_overlap = True/False (removes # from tokens)\n",
    "# # insert blank row after every sentence (after every PUNCT)\n",
    "\n",
    "# import os, sys\n",
    "# import csv\n",
    "\n",
    "# #giving the path of the directory\n",
    "# path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "# dirs = os.listdir(path)\n",
    "\n",
    "# # for every file in the directory\n",
    "# for file in dirs:\n",
    "#     filename = path+str(file)\n",
    "\n",
    "#     if filename.endswith(\"_1_Stanza_format.txt\"):\n",
    "#         filename_in = filename\n",
    "#         filename_out = filename_in.replace(\"_1_Stanza_format.txt\", \"_2_Stanza_segments.txt\")\n",
    "#         #print (\"in = \"+ filename_in)\n",
    "#         #print (\"out = \"+ filename_out)\n",
    "\n",
    "#         fields = []\n",
    "#         rows = []\n",
    "#         result_list = []\n",
    "#         sent_id = 1\n",
    "\n",
    "#         with open(filename_in, 'r',encoding=\"utf-8\") as csvfile:\n",
    "#             csvreader = csv.reader(csvfile)\n",
    "#             reader = csv.DictReader(csvfile, delimiter=',')\n",
    "            \n",
    "#             result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "# #             print(result_list)\n",
    "            \n",
    "#             for row in reader:\n",
    "#                 word_index = 1\n",
    "#                 file_name= filename_in[-16:-9]\n",
    "#                 begin_time = row[\"Begin_Time-hh:mm:ss.ms\"]\n",
    "#                 end_time = row[\"End_Time-hh:mm:ss.ms\"]\n",
    "#                 duration = row[\"Duration-hh:mm:ss.ms\"]\n",
    "#                 speaker_name = row[\"speaker_id\"]\n",
    "#                 rowdata = str(row['data']).strip()\n",
    "#                 word_list = rowdata.split(' ')\n",
    "  \n",
    "#                 flag = False\n",
    "# #                 for word in word_list:\n",
    "# #                     if word == \"#\":\n",
    "# #                         flag = True\n",
    "                \n",
    "#                 result_list.append([(\"# begin_time = \" + str(begin_time)), \"\"])\n",
    "#                 result_list.append([(\"# end_time = \" + str(end_time)), \"\"])\n",
    "#                 result_list.append([(\"# duration = \" + str(duration)), \"\"])\n",
    "#                 result_list.append([(\"# speaker_id = \" + str(speaker_name)), \"\"])\n",
    "# #                 result_list.append([(\"# contains_overlap = \"+str(flag)), \"\"])\n",
    "                \n",
    "#                 i = 0\n",
    "#                 N = len(word_list)\n",
    "\n",
    "#                 for word in word_list:\n",
    "#                     i = i + 1\n",
    "#                     print(word)\n",
    "#                     #if word != \"\": #this line creates a check for double spaces - not required now, as double speces eliminated by checks.py\n",
    "#                     if word == \"#\":\n",
    "#                         flag = True\n",
    "#                         continue\n",
    "                    \n",
    "#                     if word == \"।\" and i < N:\n",
    "#                         result_list.append([(\"# contains_overlap = \"+str(flag)), \"\"])\n",
    "#                         result_list.append([word_index, word])\n",
    "#                         result_list.append([\"\", \"\"])\n",
    "                        \n",
    "#                         word_index = 1\n",
    "#                         sent_id = sent_id+1        \n",
    "#                         result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "                    \n",
    "# #                         result_list.append([(\"# begin_time = \" + str(begin_time)),\"\"])\n",
    "# #                         result_list.append([(\"# end_time = \" + str(end_time)),\"\"])\n",
    "# #                         result_list.append([(\"# duration = \" + str(duration)),\"\"])\n",
    "# #                         result_list.append([(\"# speaker_id = \" + str(speaker_name)),\"\"])\n",
    "# #                         result_list.append([(\"# contains_overlap = \"+str(flag)), \"\"])\n",
    "# #                         print(result_list)\n",
    "#                     elif word == \"।\":\n",
    "#                         result_list.append([word_index, word])\n",
    "#                         result_list.append([\"\", \"\"])\n",
    "                        \n",
    "#                         word_index = 1\n",
    "#                         sent_id = sent_id+1        \n",
    "#                         result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "# #                         print(result_list)\n",
    "#                     else:\n",
    "#                         result_list.append([word_index, word])\n",
    "# #                         print(result_list)\n",
    "#                         word_index = word_index+1\n",
    "# #                 print(i)            \n",
    "\n",
    "#         with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "#             # creating a csv writer object\n",
    "#             csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "#             csvwriter.writerows(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d6f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # one word in a row with space as a delimiter (including annotation tags)\n",
    "# # each token/word given an index\n",
    "# # sentence segmentation with purnaviram as delimiter\n",
    "# # sent_id added to each sentence\n",
    "# # Metadata added as comments before every sentence: begin_time, end_time, duration, speaker_id, contains_overlap\n",
    "# # contains_overlap = True/False (removes # from tokens)\n",
    "# # insert blank row after every sentence (after every PUNCT)\n",
    "\n",
    "# import os, sys\n",
    "# import csv\n",
    "\n",
    "# #giving the path of the directory\n",
    "# path = \"E:/Cognitive_Science/Project/DialogueCorpus/syntactic_analysis/Stanza_Pipeline/temp/\"\n",
    "# dirs = os.listdir(path)\n",
    "\n",
    "# # for every file in the directory\n",
    "# for file in dirs:\n",
    "#     filename = path+str(file)\n",
    "\n",
    "#     if filename.endswith(\"_1_Stanza_format.txt\"):\n",
    "#         filename_in = filename\n",
    "#         filename_out = filename_in.replace(\"_1_Stanza_format.txt\", \"_2_Stanza_segments.txt\")\n",
    "#         #print (\"in = \"+ filename_in)\n",
    "#         #print (\"out = \"+ filename_out)\n",
    "\n",
    "#         fields = []\n",
    "#         rows = []\n",
    "#         result_list = []\n",
    "#         sent_id = 1\n",
    "        \n",
    "#         with open(filename_in, 'r',encoding=\"utf-8\") as csvfile:\n",
    "#             csvreader = csv.reader(csvfile)\n",
    "#             reader = csv.DictReader(csvfile, delimiter=',')\n",
    "            \n",
    "# #             result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "# #             print(result_list)\n",
    "            \n",
    "#             for row in reader:\n",
    "#                 word_index = 1\n",
    "#                 file_name= filename_in[-16:-9]\n",
    "#                 begin_time = row[\"Begin_Time-hh:mm:ss.ms\"]\n",
    "#                 end_time = row[\"End_Time-hh:mm:ss.ms\"]\n",
    "#                 duration = row[\"Duration-hh:mm:ss.ms\"]\n",
    "#                 speaker_name = row[\"speaker_id\"]\n",
    "#                 rowdata = str(row['data']).strip()\n",
    "#                 word_list = rowdata.split(' ')\n",
    "  \n",
    "#                 flag = False\n",
    "#                 ol= False\n",
    "\n",
    "#                 i = 0\n",
    "#                 N = len(word_list)\n",
    "#                 temp_list = []\n",
    "#                 for word in word_list:\n",
    "#                     i = i + 1\n",
    "#                     print(word)\n",
    "#                     #if word != \"\": #this line creates a check for double spaces - not required now, as double speces eliminated by checks.py\n",
    "#                     if word == \"#\":\n",
    "#                         if flag == False:\n",
    "            \n",
    "#                             flag = True\n",
    "#                             ol = True\n",
    "#                         else:\n",
    "#                             flag = False\n",
    "                        \n",
    "                \n",
    "#                     elif word == \"।\":\n",
    "#                         temp_list.append([word_index, word])\n",
    "                        \n",
    "#                         result_list.append([(\"# sent_id = \" + str(sent_id)),\"\"])\n",
    "#                         result_list.append([(\"# begin_time = \" + str(begin_time)), \"\"])\n",
    "#                         result_list.append([(\"# end_time = \" + str(end_time)), \"\"])\n",
    "#                         result_list.append([(\"# duration = \" + str(duration)), \"\"])\n",
    "#                         result_list.append([(\"# speaker_id = \" + str(speaker_name)), \"\"])\n",
    "#                         if flag ==True or ol ==True:\n",
    "#                             x= True\n",
    "#                         else:\n",
    "#                             x= False\n",
    "#                         result_list.append([(\"# contains_overlap = \"+str(x)), \"\"])\n",
    "#                         result_list.extend(temp_list)\n",
    "#                         result_list.append([\"\", \"\"])\n",
    "                        \n",
    "#                         temp_list = []\n",
    "#                         word_index = 1\n",
    "#                         sent_id = sent_id+1        \n",
    "#                         if flag == True:\n",
    "#                             ol= True\n",
    "#                         else:\n",
    "#                             ol = False\n",
    "                    \n",
    "#                     else:\n",
    "#                         temp_list.append([word_index, word])\n",
    "# #                         print(result_list)\n",
    "#                         word_index = word_index+1\n",
    "          \n",
    "\n",
    "#         with open(filename_out, 'w',encoding=\"utf-8\", newline='') as csvfile:\n",
    "#             # creating a csv writer object\n",
    "#             csvwriter = csv.writer(csvfile, delimiter = '\\t')\n",
    "#             csvwriter.writerows(result_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
